import requests
import json
import logging
import re
import time
from typing import Dict, Any, Optional, List, Callable, Tuple
from requests.adapters import HTTPAdapter
from requests.packages.urllib3.util.retry import Retry
from configparser import ConfigParser
from .base_service import BaseService
from .exceptions import LlmApiServiceException, NetworkException, RateLimitException
from .retry_manager import RetryManager


class LlmApiService(BaseService):
    """Service pour g√©rer les communications avec les API LLM."""
    
    # Prompt par d√©faut pour la g√©n√©ration de titre
    DEFAULT_TITLE_PROMPT = """En te basant sur l'historique de conversation suivant, g√©n√®re un titre court et descriptif (maximum 10 mots) qui r√©sume le sujet principal. R√©ponds UNIQUEMENT avec le titre, sans guillemets ni pr√©fixe comme "Titre :".

### Historique
{history}"""
    
    def __init__(self, config: Dict[str, Any], logger: Optional[logging.Logger] = None):
        """
        Initialise le service LLM API.
        
        Args:
            config: Dictionnaire de configuration contenant les mod√®les LLM
            logger: Logger optionnel
        """
        super().__init__(config, logger)
        self._llm_models = config.get('models', {})
        self._default_llm_id = config.get('default_id', None)
        self._setup_http_session()
        
        # Initialiser le RetryManager si on a plusieurs mod√®les
        if self._llm_models:
            self.retry_manager = RetryManager(
                endpoints=list(self._llm_models.keys()),
                max_retries=6,  # Plus de tentatives
                initial_backoff=1.0,
                max_backoff=30.0,
                backoff_multiplier=2.0,
                jitter=True,
                failure_threshold=3,  # Circuit breaker apr√®s 3 √©checs
                recovery_time=120  # 2 minutes avant de r√©essayer
            )
        else:
            self.retry_manager = None
        
        # Callbacks pour le suivi des erreurs
        self.error_callbacks = []
        
    def validate_config(self):
        """Valide la configuration du service LLM."""
        # La validation sera faite lors de l'appel aux m√©thodes
        pass
    
    def _setup_http_session(self):
        """Configure une session HTTP de base sans retry (g√©r√© par RetryManager)."""
        self.session = requests.Session()
        
        # IMPORTANT: D√©sactiver explicitement l'utilisation des variables d'environnement proxy
        # pour cette session si aucun proxy n'est configur√© dans le mod√®le
        self.session.trust_env = False
        
        # Pas de retry automatique ici, le RetryManager s'en charge
        adapter = HTTPAdapter(max_retries=0)
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)
    
    def _build_curl_command(self, url: str, headers: Dict[str, str], payload: Dict[str, Any], ssl_verify: bool, proxies: Optional[Dict[str, str]]) -> str:
        """
        Construit la commande curl √©quivalente pour d√©bogage.
        
        Args:
            url: URL de la requ√™te
            headers: Headers HTTP
            payload: Payload JSON
            ssl_verify: V√©rification SSL
            proxies: Configuration proxy
            
        Returns:
            Commande curl format√©e
        """
        cmd = f"curl -X POST '{url}'"
        
        # Ajouter les headers
        for key, value in headers.items():
            # Masquer les tokens sensibles
            if key.lower() == 'authorization' and 'Bearer' in value:
                masked_value = value[:20] + '...' + value[-10:] if len(value) > 30 else value
                cmd += f" \\\n  -H '{key}: {masked_value}'"
            else:
                cmd += f" \\\n  -H '{key}: {value}'"
        
        # Ajouter le payload
        if payload:
            payload_str = json.dumps(payload, ensure_ascii=False)
            # Limiter la taille du payload affich√©
            if len(payload_str) > 500:
                payload_str = payload_str[:500] + '...'
            cmd += f" \\\n  -d '{payload_str}'"
        
        # Ajouter les options SSL
        if not ssl_verify:
            cmd += " \\\n  -k"  # Ignorer les erreurs SSL
        
        # Ajouter le proxy si configur√©
        if proxies:
            if 'https' in proxies:
                cmd += f" \\\n  -x {proxies['https']}"
            elif 'http' in proxies:
                cmd += f" \\\n  -x {proxies['http']}"
        
        return cmd
    
    def _get_proxy_config(self, config: Dict[str, Any]) -> Optional[Dict[str, str]]:
        """Extrait la configuration proxy d'un mod√®le LLM.
        
        Args:
            config: Configuration du mod√®le LLM
            
        Returns:
            Dict avec les proxies ou None si pas de proxy configur√©
        """
        proxy_http = config.get('proxy_http')
        proxy_https = config.get('proxy_https')
        
        if not proxy_http and not proxy_https:
            self.logger.debug("Pas de configuration proxy d√©tect√©e")
            return None
        
        proxies = {}
        if proxy_http:
            proxies['http'] = proxy_http
            self.logger.info(f"Proxy HTTP configur√©: {proxy_http}")
        if proxy_https:
            proxies['https'] = proxy_https
            self.logger.info(f"Proxy HTTPS configur√©: {proxy_https}")
        
        # G√©rer les exclusions no_proxy
        no_proxy = config.get('proxy_no_proxy')
        if no_proxy:
            # Configurer les exclusions via les variables d'environnement
            # car requests utilise ces variables pour les exclusions
            import os
            os.environ['NO_PROXY'] = no_proxy
            os.environ['no_proxy'] = no_proxy
            self.logger.info(f"Exclusions proxy (NO_PROXY): {no_proxy}")
        
        self.logger.info(f"Configuration proxy finale: {proxies}")
        return proxies
    
    def get_available_models(self) -> List[Dict[str, Any]]:
        """Retourne la liste des mod√®les LLM disponibles."""
        models = [
            {'id': llm_id, 'name': model['name'], 'default': model.get('default', False)}
            for llm_id, model in self._llm_models.items()
        ]
        
        # Ajouter le statut de sant√© si disponible
        if self.retry_manager:
            health_status = self.retry_manager.get_health_status()
            for model in models:
                if model['id'] in health_status:
                    model['health'] = health_status[model['id']]
        
        return models
    
    def get_endpoints_health(self) -> Optional[Dict[str, Any]]:
        """Retourne le statut de sant√© de tous les endpoints."""
        if self.retry_manager:
            return self.retry_manager.get_health_status()
        return None
    
    def reset_endpoint_health(self, endpoint_id: str):
        """R√©initialise le statut de sant√© d'un endpoint."""
        if self.retry_manager:
            self.retry_manager.reset_endpoint(endpoint_id)
    
    def _build_openai_request(self, api_url: str, model: str, messages: List[Dict[str, str]], 
                              stream: bool = False, temperature: float = None, 
                              max_tokens: int = None) -> tuple:
        """
        Construit une requ√™te pour une API compatible OpenAI.
        
        Returns:
            tuple: (target_url, payload)
        """
        payload = {
            "model": model,
            "messages": messages,
            "stream": stream
        }
        
        # Ajouter les param√®tres optionnels s'ils sont d√©finis
        if temperature is not None:
            payload['temperature'] = temperature
        if max_tokens is not None:
            payload['max_tokens'] = max_tokens
            
        # Construire l'URL compl√®te
        target_url = api_url.rstrip('/')
        if not target_url.endswith('/chat/completions'):
            target_url += '/chat/completions' if '/v1' in target_url else '/v1/chat/completions'
            
        return target_url, payload
    
    def _prepare_request(self, chat_history: List[Dict[str, str]], stream: bool = False, llm_id: Optional[str] = None) -> tuple:
        """
        Pr√©pare les donn√©es pour la requ√™te LLM.
        
        Args:
            chat_history: Historique de la conversation
            stream: Mode streaming ou non
            llm_id: ID du mod√®le √† utiliser (optionnel, fallback sur le d√©faut)
            
        Returns:
            tuple: (url, headers, payload, ssl_verify)
        """
        # S√©lectionner le mod√®le √† utiliser
        target_llm_id = llm_id if llm_id and llm_id in self._llm_models else self._default_llm_id
        if not target_llm_id:
            raise LlmApiServiceException('No default or valid LLM configured.')
        
        self.logger.debug(f"Pr√©paration de la requ√™te pour le mod√®le: {target_llm_id}")
        final_config = self._llm_models[target_llm_id]
        
        if not final_config.get('url') or not final_config.get('model'):
            raise LlmApiServiceException(f'LLM configuration is incomplete for model {target_llm_id}')
        
        headers = {"Content-Type": "application/json"}
        if final_config.get('apikey'):
            headers["Authorization"] = f"Bearer {final_config['apikey']}"
        
        if final_config.get('api_type') == "openai":
            # Utiliser la m√©thode factoris√©e
            target_url, payload = self._build_openai_request(
                api_url=final_config['url'],
                model=final_config['model'],
                messages=chat_history,
                stream=stream,
                temperature=final_config.get('temperature'),
                max_tokens=final_config.get('max_tokens')
            )
        else:  # ollama
            prompt = "\n\n".join([f"{msg['role'].upper()}: {msg['content']}" for msg in chat_history])
            payload = {
                "model": final_config['model'],
                "prompt": prompt,
                "stream": stream
            }
            # Ajouter les param√®tres optionnels pour Ollama aussi
            if final_config.get('temperature') is not None:
                payload['temperature'] = final_config['temperature']
                
            target_url = final_config['url'].rstrip('/') + "/api/generate"
        
        self.logger.debug(f"URL finale construite: {target_url}")
        self.logger.debug(f"Type d'API: {final_config.get('api_type')}")
        self.logger.debug(f"Nombre de messages dans l'historique: {len(chat_history)}")
        
        return target_url, headers, payload, final_config.get('ssl_verify', True)
    
    def register_error_callback(self, callback: Callable[[str, int, float], None]):
        """Enregistre un callback pour les notifications d'erreur."""
        self.error_callbacks.append(callback)
    
    def _notify_error(self, message: str, attempt: int = 0, wait_time: float = 0):
        """Notifie les callbacks d'une erreur."""
        for callback in self.error_callbacks:
            try:
                callback(message, attempt, wait_time)
            except Exception as e:
                self.logger.warning(f"Erreur dans le callback d'erreur: {e}")
    
    def send_to_llm(self, chat_history: List[Dict[str, str]], stream: bool = False, llm_id: Optional[str] = None, use_failover: bool = True) -> Dict[str, Any]:
        """
        Envoie l'historique du chat au LLM et retourne la r√©ponse.
        
        Args:
            chat_history: Liste des messages de la conversation
            stream: Si True, utilise le mode streaming
            llm_id: ID du mod√®le √† utiliser (optionnel)
            
        Returns:
            Dict contenant la r√©ponse ou une erreur
        """
        # Log imm√©diat du lancement
        target_llm = llm_id if llm_id else self._default_llm_id
        model_name = self._llm_models[target_llm].get('name', target_llm) if target_llm in self._llm_models else 'inconnu'
        
        # PRINT DIRECT pour √™tre s√ªr que √ßa s'affiche
        print(f"\n{'='*60}")
        print(f"üöÄ NOUVELLE REQU√äTE LLM - LANCEMENT IMM√âDIAT")
        print(f"üì° Serveur cible: {target_llm} ({model_name})")
        print(f"üîÑ Failover: {'Activ√©' if use_failover else 'D√©sactiv√©'}")
        print(f"üìä Mode: {'STREAMING' if stream else 'STANDARD (NON-STREAMING)'}")
        print(f"üí¨ Messages dans l'historique: {len(chat_history)}")
        if not stream:
            print(f"‚ö†Ô∏è  ATTENTION: Mode non-streaming, risque de timeout sur les longues r√©ponses!")
        print(f"{'='*60}\n")
        
        # Logs normaux aussi
        self.logger.info(f"\n{'='*60}")
        self.logger.info(f"üöÄ NOUVELLE REQU√äTE LLM - LANCEMENT IMM√âDIAT")
        self.logger.info(f"üì° Serveur cible: {target_llm} ({model_name})")
        self.logger.info(f"üîÑ Failover: {'Activ√©' if use_failover else 'D√©sactiv√©'}")
        self.logger.info(f"üìä Mode: {'Streaming' if stream else 'Standard'}")
        self.logger.info(f"üí¨ Messages dans l'historique: {len(chat_history)}")
        self.logger.info(f"{'='*60}\n")
        
        # Si un mod√®le sp√©cifique est demand√©, essayer d'abord celui-ci
        if llm_id and llm_id in self._llm_models:
            try:
                self.logger.info(f"Utilisation du mod√®le sp√©cifiquement s√©lectionn√©: {llm_id}")
                return self._send_to_llm_internal(chat_history, stream, llm_id)
            except Exception as e:
                self.logger.warning(f"√âchec du mod√®le s√©lectionn√© {llm_id}: {str(e)}")
                # Si failover d√©sactiv√© ou pas de failover disponible, propager l'erreur
                if not use_failover or not self.retry_manager or len(self._llm_models) <= 1:
                    self.logger.info(f"Failover d√©sactiv√© (use_failover={use_failover}) ou non disponible")
                    raise  # Pas de failover, propager l'erreur
                
        # Si on a un retry manager et plusieurs endpoints ET que le failover est activ√©
        if use_failover and self.retry_manager and len(self._llm_models) > 1:
            def execute_request(endpoint_id: str) -> Dict[str, Any]:
                # Ne pas r√©essayer le mod√®le qui vient d'√©chouer
                if llm_id and endpoint_id == llm_id:
                    raise Exception(f"Mod√®le {endpoint_id} d√©j√† essay√©")
                return self._send_to_llm_internal(chat_history, stream, endpoint_id)
            
            def on_retry(attempt: int, endpoint: str, wait_time: float):
                # Ajuster le compteur de tentatives si on a d√©j√† essay√© le mod√®le s√©lectionn√©
                actual_attempt = attempt + (1 if llm_id and llm_id in self._llm_models else 0)
                msg = f"Tentative {actual_attempt}: √âchec sur {endpoint}. Nouvelle tentative dans {wait_time:.1f}s..."
                self._notify_error(msg, actual_attempt, wait_time)
            
            def on_endpoint_switch(new_endpoint: str):
                model_name = self._llm_models[new_endpoint].get('name', new_endpoint)
                msg = f"Basculement vers le mod√®le: {model_name}"
                self._notify_error(msg, 0, 0)
            
            try:
                return self.retry_manager.execute_with_retry(
                    execute_request,
                    on_retry=on_retry,
                    on_endpoint_switch=on_endpoint_switch
                )
            except Exception as e:
                # Ajouter le statut de sant√© des endpoints dans l'erreur
                health_status = self.retry_manager.get_health_status()
                self.logger.error(f"Tous les endpoints ont √©chou√©. Statut: {health_status}")
                self._notify_error(f"Erreur critique: Tous les serveurs LLM sont indisponibles", -1, 0)
                raise LlmApiServiceException(f"Tous les endpoints LLM ont √©chou√©: {str(e)}")
        else:
            # Pas de retry manager, utiliser l'ancienne m√©thode
            return self._send_to_llm_internal(chat_history, stream, llm_id)
    
    def _send_to_llm_internal(self, chat_history: List[Dict[str, str]], stream: bool = False, llm_id: Optional[str] = None) -> Dict[str, Any]:
        """
        M√©thode interne pour envoyer au LLM (utilis√©e par le retry manager).
        """
        try:
            target_url, headers, payload, ssl_verify = self._prepare_request(chat_history, stream, llm_id)
            
            # R√©cup√©rer la config du mod√®le utilis√©
            target_llm_id = llm_id if llm_id and llm_id in self._llm_models else self._default_llm_id
            if not target_llm_id:
                raise LlmApiServiceException('No model configured')
            current_config = self._llm_models[target_llm_id]
            
            if not ssl_verify:
                import urllib3
                urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
            
            token_count = self._count_tokens_for_history(chat_history)
            self.logger.debug(f"Estimated token count: {token_count}")
            
            # Timeout adaptatif : plus court pour les premi√®res tentatives
            timeout = current_config.get('timeout_seconds', 300)
            
            # Configurer le proxy si d√©fini
            # IMPORTANT: Si pas de proxy configur√©, on met None (pas {}) pour ignorer les variables d'environnement
            proxy_config = self._get_proxy_config(current_config)
            proxies = proxy_config if proxy_config else None
            
            # Log d√©taill√© de la requ√™te (plus concis)
            self.logger.info(f"Requ√™te LLM: {llm_id} -> {current_config.get('model')} (timeout={timeout}s, proxy={'Oui' if proxies else 'Non'})")
            self.logger.debug(f"URL cible: {target_url}")
            self.logger.debug(f"SSL verify: {ssl_verify}")
            if proxies:
                self.logger.debug(f"D√©tails proxy: {proxies}")
            self.logger.debug(f"Headers: {headers}")
            # Tronquer le payload pour √©viter de polluer les logs
            if payload:
                payload_str = json.dumps(payload, ensure_ascii=False)
                if len(payload_str) > 500:
                    self.logger.debug(f"Payload (tronqu√©): {payload_str[:500]}...")
                    self.logger.debug(f"Taille totale du payload: {len(payload_str)} caract√®res")
                else:
                    self.logger.debug(f"Payload: {payload_str}")
            else:
                self.logger.debug("Payload: None")
            self.logger.debug(f"=== COMMANDE CURL √âQUIVALENTE ===")
            curl_cmd = self._build_curl_command(target_url, headers, payload, ssl_verify, proxies)
            self.logger.debug(f"{curl_cmd}")
            self.logger.debug(f"===================================")
            
            # Gestion du proxy : forcer explicitement l'absence de proxy si non configur√©
            if proxies:
                self.session.trust_env = True
                self.logger.debug("trust_env activ√© car proxy configur√©")
                final_proxies = proxies
            else:
                self.session.trust_env = False
                # IMPORTANT: Forcer explicitement l'absence de proxy avec des cha√Ænes vides
                # car proxies=None peut encore utiliser les variables d'environnement
                final_proxies = {"http": "", "https": ""}
                self.logger.debug("Proxy forc√© √† vide pour ignorer les variables d'environnement")
            
            response = self.session.post(
                target_url,
                headers=headers,
                json=payload,
                verify=ssl_verify,
                timeout=timeout,
                proxies=final_proxies
            )
            
            self.logger.info(f"R√©ponse re√ßue: Status {response.status_code}")
            self.logger.debug(f"Headers de r√©ponse: {response.headers}")
            
            # G√©rer les erreurs HTTP
            if response.status_code == 429:
                retry_after = response.headers.get('Retry-After', 60)
                raise RateLimitException(
                    f"Rate limit exceeded. Retry after {retry_after} seconds",
                    int(retry_after)
                )
            
            # V√©rifier le status code avant raise_for_status pour un meilleur logging
            if response.status_code >= 500:
                self.logger.warning(f"Erreur serveur d√©tect√©e: {response.status_code}")
            
            response.raise_for_status()
            
            # Log de la r√©ponse brute pour d√©boguer
            response_text = response.text
            self.logger.info(f"Taille de la r√©ponse: {len(response_text)} octets")
            
            # Ne logger le contenu que si c'est court ou en cas d'erreur
            if len(response_text) < 500:
                self.logger.debug(f"R√©ponse brute: {response_text}")
            else:
                self.logger.debug(f"R√©ponse trop longue pour √™tre logg√©e ({len(response_text)} octets)")
            
            # Parser la r√©ponse selon le type d'API
            if current_config.get('api_type') == "openai":
                result = response.json()
                # Ne pas logger le JSON complet pour √©viter de polluer les logs
                self.logger.debug(f"Structure de la r√©ponse: choices={len(result.get('choices', []))} items")
                
                if 'choices' in result and result['choices']:
                    choice = result['choices'][0]
                    
                    # V√©rifier si la r√©ponse a √©t√© tronqu√©e
                    finish_reason = choice.get('finish_reason')
                    if finish_reason == 'length':
                        self.logger.warning("‚ö†Ô∏è R√âPONSE TRONQU√âE: le LLM a atteint la limite de tokens!")
                        print("‚ö†Ô∏è ATTENTION: La r√©ponse du LLM a √©t√© tronqu√©e (limite de tokens atteinte)")
                    elif finish_reason == 'stop':
                        self.logger.info("‚úÖ R√©ponse compl√®te re√ßue")
                    else:
                        self.logger.warning(f"finish_reason inhabituel: {finish_reason}")
                    
                    message_content = choice.get('message', {}).get('content', '')
                    
                    if not message_content:
                        self.logger.error(f"‚ùå Contenu vide! Choice: {json.dumps(choice, ensure_ascii=False)}")
                        return {'error': 'Le LLM a retourn√© une r√©ponse vide'}
                    
                    self.logger.info(f"‚úÖ R√©ponse re√ßue: {len(message_content)} caract√®res")
                    return {'response': message_content}
                else:
                    self.logger.error(f"Format inattendu: {json.dumps(result, ensure_ascii=False)[:500]}")
                    return {'error': 'Unexpected response format from LLM'}
            else:  # ollama
                result = response.json()
                if 'response' in result:
                    return {'response': result['response']}
                else:
                    return {'error': 'Unexpected response format from LLM'}
                    
        except RateLimitException:
            raise  # Re-raise pour que l'appelant puisse g√©rer
        except requests.exceptions.ProxyError as e:
            self.logger.error(f"=== ERREUR PROXY ===")
            self.logger.error(f"Erreur de proxy pour {llm_id}: {e}")
            self.logger.error(f"Configuration proxy utilis√©e: {proxies if 'proxies' in locals() else 'Non d√©fini'}")
            self.logger.error(f"URL tent√©e: {target_url if 'target_url' in locals() else 'Non d√©fini'}")
            self._notify_error(f"Erreur de proxy sur {llm_id}: {str(e)}", -1, 0)
            raise NetworkException(f"Erreur de proxy: {str(e)}")
        except requests.exceptions.SSLError as e:
            self.logger.error(f"=== ERREUR SSL ===")
            self.logger.error(f"Erreur SSL pour {llm_id}: {e}")
            self.logger.error(f"SSL verify √©tait: {ssl_verify if 'ssl_verify' in locals() else 'Non d√©fini'}")
            self._notify_error(f"Erreur SSL sur {llm_id}", -1, 0)
            raise NetworkException(f"Erreur SSL: {str(e)}")
        except requests.exceptions.Timeout as e:
            self.logger.error(f"=== TIMEOUT ===")
            self.logger.error(f"Timeout calling LLM {llm_id}: {e}")
            self.logger.error(f"Timeout configur√©: {timeout if 'timeout' in locals() else 'Non d√©fini'}s")
            # Notifier l'erreur de timeout
            self._notify_error(f"Timeout apr√®s {timeout if 'timeout' in locals() else '?'}s sur {llm_id}", -1, 0)
            raise NetworkException(f"Timeout apr√®s {timeout if 'timeout' in locals() else '?'} secondes")
        except requests.exceptions.ConnectionError as e:
            self.logger.error(f"=== ERREUR DE CONNEXION ===")
            self.logger.error(f"Connection error calling LLM {llm_id}: {e}")
            self.logger.error(f"URL: {target_url if 'target_url' in locals() else 'Non d√©fini'}")
            self.logger.error(f"Proxy: {proxies if 'proxies' in locals() else 'Aucun'}")
            self._notify_error(f"Erreur de connexion sur {llm_id}", -1, 0)
            raise NetworkException(f"Erreur de connexion: {str(e)}")
        except requests.exceptions.HTTPError as e:
            # Traitement sp√©cifique des erreurs HTTP
            if hasattr(e.response, 'status_code'):
                status_code = e.response.status_code
                if status_code == 504:
                    self.logger.error(f"=== ERREUR 504 GATEWAY TIMEOUT ===")
                    self.logger.error(f"Gateway timeout pour {llm_id}: Le serveur interm√©diaire n'a pas re√ßu de r√©ponse dans le d√©lai imparti")
                    self.logger.error(f"URL: {target_url if 'target_url' in locals() else 'Non d√©fini'}")
                    self.logger.error(f"Timeout configur√©: {timeout if 'timeout' in locals() else 'Non d√©fini'}s")
                    self.logger.error(f"SUGGESTIONS:")
                    self.logger.error(f"  1. Le serveur LLM est surcharg√© ou indisponible")
                    self.logger.error(f"  2. Augmentez timeout_seconds dans la configuration (actuellement: {timeout if 'timeout' in locals() else '?'}s)")
                    self.logger.error(f"  3. Essayez un autre mod√®le LLM si disponible")
                    self._notify_error(f"Gateway timeout (504) sur {llm_id} apr√®s {timeout if 'timeout' in locals() else '?'}s", -1, 0)
                    raise NetworkException(f"Gateway timeout (504): Le serveur {llm_id} est surcharg√© ou indisponible")
                elif status_code == 502:
                    self.logger.error(f"=== ERREUR 502 BAD GATEWAY ===")
                    self.logger.error(f"Bad gateway pour {llm_id}: Le serveur proxy a re√ßu une r√©ponse invalide")
                    self._notify_error(f"Bad gateway (502) sur {llm_id}", -1, 0)
                    raise NetworkException(f"Bad gateway (502): Probl√®me de communication avec {llm_id}")
                elif status_code == 503:
                    self.logger.error(f"=== ERREUR 503 SERVICE UNAVAILABLE ===")
                    self.logger.error(f"Service indisponible pour {llm_id}")
                    self._notify_error(f"Service indisponible (503) sur {llm_id}", -1, 0)
                    raise NetworkException(f"Service indisponible (503): {llm_id} est temporairement indisponible")
                else:
                    self.logger.error(f"=== ERREUR HTTP {status_code} ===")
                    self.logger.error(f"Erreur HTTP {status_code} pour {llm_id}: {e}")
                    raise NetworkException(f"Erreur HTTP {status_code}: {str(e)}")
            else:
                # Erreur HTTP sans status_code
                self.logger.error(f"=== ERREUR HTTP ===")
                self.logger.error(f"Erreur HTTP pour {llm_id}: {e}")
                raise NetworkException(f"Erreur HTTP: {str(e)}")
        except requests.exceptions.RequestException as e:
            self.logger.error(f"=== ERREUR R√âSEAU G√âN√âRALE ===")
            self.logger.error(f"Network error calling LLM {llm_id}: {e}")
            self.logger.error(f"Type d'erreur: {type(e).__name__}")
            raise NetworkException(f"Network error: {str(e)}")
        except Exception as e:
            self.logger.error(f"=== ERREUR INATTENDUE ===")
            self.logger.error(f"Error calling LLM {llm_id}: {e}")
            self.logger.error(f"Type d'erreur: {type(e).__name__}")
            import traceback
            self.logger.error(f"Stack trace: {traceback.format_exc()}")
            raise LlmApiServiceException(f"Error calling LLM: {str(e)}")
    
    def send_to_llm_stream(self, chat_history: List[Dict[str, str]], 
                          on_start: Optional[Callable[[], None]] = None,
                          on_chunk: Optional[Callable[[str], None]] = None,
                          on_end: Optional[Callable[[int], None]] = None,
                          on_error: Optional[Callable[[str], None]] = None,
                          llm_id: Optional[str] = None,
                          use_failover: bool = True) -> Dict[str, Any]:
        """
        Envoie l'historique au LLM en mode streaming avec callbacks.
        
        Args:
            chat_history: Liste des messages de la conversation
            on_start: Callback appel√© au d√©but du streaming
            on_chunk: Callback appel√© pour chaque chunk re√ßu
            on_end: Callback appel√© √† la fin avec le nombre total de tokens
            on_error: Callback appel√© en cas d'erreur
            llm_id: ID du mod√®le √† utiliser (optionnel)
            
        Returns:
            Dict contenant le statut ou une erreur
        """
        # Log imm√©diat du lancement streaming
        target_llm = llm_id if llm_id else self._default_llm_id
        model_name = self._llm_models[target_llm].get('name', target_llm) if target_llm in self._llm_models else 'inconnu'
        
        # PRINT DIRECT pour √™tre s√ªr que √ßa s'affiche
        print(f"\n{'='*60}")
        print(f"üöÄ NOUVELLE REQU√äTE LLM STREAMING - LANCEMENT IMM√âDIAT")
        print(f"üì° Serveur cible: {target_llm} ({model_name})")
        print(f"üîÑ Failover: {'Activ√©' if use_failover else 'D√©sactiv√©'}")
        print(f"üí¨ Messages dans l'historique: {len(chat_history)}")
        print(f"{'='*60}\n")
        
        # Logs normaux aussi
        self.logger.info(f"\n{'='*60}")
        self.logger.info(f"üöÄ NOUVELLE REQU√äTE LLM STREAMING - LANCEMENT IMM√âDIAT")
        self.logger.info(f"üì° Serveur cible: {target_llm} ({model_name})")
        self.logger.info(f"üîÑ Failover: {'Activ√©' if use_failover else 'D√©sactiv√©'}")
        self.logger.info(f"üí¨ Messages dans l'historique: {len(chat_history)}")
        self.logger.info(f"{'='*60}\n")
        
        # Si un mod√®le sp√©cifique est demand√©, essayer d'abord celui-ci
        if llm_id and llm_id in self._llm_models:
            try:
                self.logger.info(f"Utilisation du mod√®le sp√©cifiquement s√©lectionn√© (streaming): {llm_id}")
                return self._send_to_llm_stream_internal(chat_history, on_start, on_chunk, on_end, on_error, llm_id)
            except Exception as e:
                self.logger.warning(f"√âchec du mod√®le s√©lectionn√© {llm_id} en streaming: {str(e)}")
                # Si failover d√©sactiv√© ou pas de failover disponible, propager l'erreur
                if not use_failover or not self.retry_manager or len(self._llm_models) <= 1:
                    self.logger.info(f"Failover d√©sactiv√© (use_failover={use_failover}) ou non disponible")
                    raise  # Pas de failover, propager l'erreur
        
        # Si on a un retry manager et plusieurs endpoints ET que le failover est activ√©
        if use_failover and self.retry_manager and len(self._llm_models) > 1:
            def execute_stream(endpoint_id: str) -> Dict[str, Any]:
                # Ne pas r√©essayer le mod√®le qui vient d'√©chouer
                if llm_id and endpoint_id == llm_id:
                    raise Exception(f"Mod√®le {endpoint_id} d√©j√† essay√©")
                return self._send_to_llm_stream_internal(
                    chat_history, on_start, on_chunk, on_end, None, endpoint_id
                )
            
            def on_retry(attempt: int, endpoint: str, wait_time: float):
                # Ajuster le compteur de tentatives si on a d√©j√† essay√© le mod√®le s√©lectionn√©
                actual_attempt = attempt + (1 if llm_id and llm_id in self._llm_models else 0)
                msg = f"Tentative {actual_attempt}: √âchec sur {endpoint}. Nouvelle tentative dans {wait_time:.1f}s..."
                self._notify_error(msg, actual_attempt, wait_time)
                if on_error:
                    on_error(msg)
            
            def on_endpoint_switch(new_endpoint: str):
                model_name = self._llm_models[new_endpoint].get('name', new_endpoint)
                msg = f"Basculement vers le mod√®le: {model_name}"
                self._notify_error(msg, 0, 0)
            
            try:
                return self.retry_manager.execute_with_retry(
                    execute_stream,
                    on_retry=on_retry,
                    on_endpoint_switch=on_endpoint_switch
                )
            except Exception as e:
                health_status = self.retry_manager.get_health_status()
                self.logger.error(f"Tous les endpoints ont √©chou√© en streaming. Statut: {health_status}")
                if on_error:
                    on_error(f"Erreur critique: Tous les serveurs LLM sont indisponibles")
                raise LlmApiServiceException(f"Tous les endpoints LLM ont √©chou√©: {str(e)}")
        else:
            # Pas de retry manager, utiliser l'ancienne m√©thode
            return self._send_to_llm_stream_internal(
                chat_history, on_start, on_chunk, on_end, on_error, llm_id
            )
    
    def _send_to_llm_stream_internal(self, chat_history: List[Dict[str, str]], 
                                    on_start: Optional[Callable[[], None]] = None,
                                    on_chunk: Optional[Callable[[str], None]] = None,
                                    on_end: Optional[Callable[[int], None]] = None,
                                    on_error: Optional[Callable[[str], None]] = None,
                                    llm_id: Optional[str] = None) -> Dict[str, Any]:
        """
        M√©thode interne pour le streaming (utilis√©e par le retry manager).
        """
        try:
            target_url, headers, payload, ssl_verify = self._prepare_request(chat_history, stream=True, llm_id=llm_id)
            
            # R√©cup√©rer la config du mod√®le utilis√©
            target_llm_id = llm_id if llm_id and llm_id in self._llm_models else self._default_llm_id
            if not target_llm_id:
                raise LlmApiServiceException('No model configured')
            current_config = self._llm_models[target_llm_id]
            
            if not ssl_verify:
                import urllib3
                urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
            
            token_count = self._count_tokens_for_history(chat_history)
            self.logger.debug(f"Estimated token count: {token_count}")
            
            # Timeout adaptatif : plus court pour les premi√®res tentatives
            timeout = current_config.get('timeout_seconds', 300)
            
            # Configurer le proxy si d√©fini
            # IMPORTANT: Si pas de proxy configur√©, on met None (pas {}) pour ignorer les variables d'environnement
            proxy_config = self._get_proxy_config(current_config)
            proxies = proxy_config if proxy_config else None
            
            # Log d√©taill√© pour le streaming (plus concis)
            self.logger.info(f"Requ√™te LLM STREAMING: {llm_id} -> {current_config.get('model')} (timeout={timeout}s, proxy={'Oui' if proxies else 'Non'})")
            self.logger.debug(f"URL cible: {target_url}")
            if proxies:
                self.logger.debug(f"D√©tails proxy: {proxies}")
            self.logger.debug(f"Headers: {headers}")
            # Tronquer le payload pour √©viter de polluer les logs
            if payload:
                payload_str = json.dumps(payload, ensure_ascii=False)
                if len(payload_str) > 500:
                    self.logger.debug(f"Payload (tronqu√©): {payload_str[:500]}...")
                    self.logger.debug(f"Taille totale du payload: {len(payload_str)} caract√®res")
                else:
                    self.logger.debug(f"Payload: {payload_str}")
            else:
                self.logger.debug("Payload: None")
            self.logger.debug(f"=== COMMANDE CURL √âQUIVALENTE (STREAMING) ===")
            curl_cmd = self._build_curl_command(target_url, headers, payload, ssl_verify, proxies)
            self.logger.debug(f"{curl_cmd}")
            self.logger.debug(f"=========================================")
            
            # Gestion du proxy : forcer explicitement l'absence de proxy si non configur√©
            if proxies:
                self.session.trust_env = True
                self.logger.debug("trust_env activ√© car proxy configur√©")
                final_proxies = proxies
            else:
                self.session.trust_env = False
                # IMPORTANT: Forcer explicitement l'absence de proxy avec des cha√Ænes vides
                # car proxies=None peut encore utiliser les variables d'environnement
                final_proxies = {"http": "", "https": ""}
                self.logger.debug("Proxy forc√© √† vide pour ignorer les variables d'environnement")
            
            response = self.session.post(
                target_url,
                headers=headers,
                json=payload,
                verify=ssl_verify,
                stream=True,
                timeout=timeout,
                proxies=final_proxies
            )
            
            self.logger.info(f"Streaming r√©ponse re√ßue: Status {response.status_code}")
            
            # G√©rer les erreurs HTTP
            if response.status_code == 429:
                retry_after = response.headers.get('Retry-After', 60)
                raise RateLimitException(
                    f"Rate limit exceeded. Retry after {retry_after} seconds",
                    int(retry_after)
                )
            
            response.raise_for_status()
            
            # Appeler le callback de d√©but
            if on_start:
                self.logger.info("Appel du callback de d√©but de streaming")
                on_start()
            
            # Parser la r√©ponse en streaming
            accumulated_content = ""
            
            for line in response.iter_lines():
                if line:
                    line_str = line.decode('utf-8')
                    
                    if current_config.get('api_type') == "openai":
                        if line_str.startswith('data: '):
                            data_str = line_str[6:]
                            if data_str == '[DONE]':
                                break
                            try:
                                data = json.loads(data_str)
                                if 'choices' in data and data['choices']:
                                    delta = data['choices'][0].get('delta', {})
                                    if 'content' in delta:
                                        content = delta['content']
                                        accumulated_content += content
                                        # Appeler le callback de chunk
                                        if on_chunk:
                                            on_chunk(content)
                            except json.JSONDecodeError:
                                self.logger.warning(f"Failed to parse streaming data: {data_str}")
                    else:  # ollama
                        try:
                            data = json.loads(line_str)
                            if 'response' in data:
                                content = data['response']
                                accumulated_content += content
                                # Appeler le callback de chunk
                                if on_chunk:
                                    on_chunk(content)
                            if data.get('done', False):
                                break
                        except json.JSONDecodeError:
                            self.logger.warning(f"Failed to parse streaming data: {line_str}")
            
            # Cr√©er l'historique final et compter les tokens
            final_history = chat_history + [{'role': 'assistant', 'content': accumulated_content}]
            total_tokens = self._count_tokens_for_history(final_history)
            
            # Appeler le callback de fin avec le comptage de tokens
            if on_end:
                on_end(total_tokens)
            
            self.logger.info(f"Streaming completed. Total content length: {len(accumulated_content)}, tokens: {total_tokens}")
            return {'response': accumulated_content, 'total_tokens': total_tokens}
            
        except RateLimitException:
            raise
        except requests.exceptions.ProxyError as e:
            self.logger.error(f"=== ERREUR PROXY (STREAMING) ===")
            self.logger.error(f"Erreur de proxy pendant streaming {llm_id}: {e}")
            self.logger.error(f"Configuration proxy: {proxies if 'proxies' in locals() else 'Non d√©fini'}")
            error_msg = f"Erreur de proxy sur {llm_id}: {str(e)}"
            if on_error:
                on_error(error_msg)
            self._notify_error(error_msg, -1, 0)
            raise NetworkException(error_msg)
        except requests.exceptions.SSLError as e:
            self.logger.error(f"=== ERREUR SSL (STREAMING) ===")
            self.logger.error(f"Erreur SSL pendant streaming {llm_id}: {e}")
            error_msg = f"Erreur SSL sur {llm_id}: {str(e)}"
            if on_error:
                on_error(error_msg)
            self._notify_error(error_msg, -1, 0)
            raise NetworkException(error_msg)
        except requests.exceptions.Timeout as e:
            self.logger.error(f"=== TIMEOUT (STREAMING) ===")
            self.logger.error(f"Timeout during streaming with {llm_id}: {e}")
            self.logger.error(f"Timeout √©tait: {timeout if 'timeout' in locals() else '?'}s")
            error_msg = f"Timeout apr√®s {timeout if 'timeout' in locals() else '?'} secondes sur {llm_id}"
            # Notifier via callback et notification globale
            if on_error:
                on_error(error_msg)
            self._notify_error(error_msg, -1, 0)
            raise NetworkException(error_msg)
        except requests.exceptions.ConnectionError as e:
            self.logger.error(f"=== ERREUR DE CONNEXION (STREAMING) ===")
            self.logger.error(f"Connection error during streaming with {llm_id}: {e}")
            self.logger.error(f"URL: {target_url if 'target_url' in locals() else '?'}")
            self.logger.error(f"Proxy: {proxies if 'proxies' in locals() else 'Aucun'}")
            error_msg = f"Erreur de connexion sur {llm_id}: {str(e)}"
            if on_error:
                on_error(error_msg)
            self._notify_error(error_msg, -1, 0)
            raise NetworkException(error_msg)
        except requests.exceptions.HTTPError as e:
            # Traitement sp√©cifique des erreurs HTTP en streaming
            if hasattr(e.response, 'status_code'):
                status_code = e.response.status_code
                if status_code == 504:
                    self.logger.error(f"=== ERREUR 504 GATEWAY TIMEOUT (STREAMING) ===")
                    self.logger.error(f"Gateway timeout streaming pour {llm_id}")
                    self.logger.error(f"Timeout configur√©: {timeout if 'timeout' in locals() else '?'}s")
                    error_msg = f"Gateway timeout (504) en streaming sur {llm_id}"
                    if on_error:
                        on_error(error_msg)
                    self._notify_error(error_msg, -1, 0)
                    raise NetworkException(error_msg)
                elif status_code == 502:
                    error_msg = f"Bad gateway (502) en streaming sur {llm_id}"
                    self.logger.error(f"=== ERREUR 502 BAD GATEWAY (STREAMING) ===")
                    if on_error:
                        on_error(error_msg)
                    self._notify_error(error_msg, -1, 0)
                    raise NetworkException(error_msg)
                elif status_code == 503:
                    error_msg = f"Service indisponible (503) en streaming sur {llm_id}"
                    self.logger.error(f"=== ERREUR 503 SERVICE UNAVAILABLE (STREAMING) ===")
                    if on_error:
                        on_error(error_msg)
                    self._notify_error(error_msg, -1, 0)
                    raise NetworkException(error_msg)
                else:
                    error_msg = f"Erreur HTTP {status_code} en streaming sur {llm_id}"
                    self.logger.error(f"=== ERREUR HTTP {status_code} (STREAMING) ===")
                    if on_error:
                        on_error(error_msg)
                    raise NetworkException(error_msg)
            else:
                error_msg = f"Erreur HTTP en streaming sur {llm_id}: {str(e)}"
                if on_error:
                    on_error(error_msg)
                raise NetworkException(error_msg)
        except requests.exceptions.RequestException as e:
            self.logger.error(f"=== ERREUR R√âSEAU (STREAMING) ===")
            self.logger.error(f"Network error during streaming with {llm_id}: {e}")
            self.logger.error(f"Type: {type(e).__name__}")
            # Appeler le callback d'erreur
            if on_error:
                on_error(str(e))
            raise NetworkException(f"Network error during streaming: {str(e)}")
        except Exception as e:
            self.logger.error(f"=== ERREUR INATTENDUE (STREAMING) ===")
            self.logger.error(f"Error during streaming with {llm_id}: {e}")
            self.logger.error(f"Type: {type(e).__name__}")
            import traceback
            self.logger.error(f"Stack trace: {traceback.format_exc()}")
            # Appeler le callback d'erreur
            if on_error:
                on_error(str(e))
            raise LlmApiServiceException(f"Error during streaming: {str(e)}")
    
    def _count_tokens_for_history(self, chat_history: List[Dict[str, str]]) -> int:
        """Compte le nombre total de tokens dans l'historique du chat."""
        try:
            total_tokens = 0
            
            for message in chat_history:
                content = message.get('content', '')
                role = message.get('role', '')
                
                # Compter les tokens du contenu
                content_tokens = self._estimate_tokens(content)
                
                # Ajouter le surco√ªt pour le r√¥le et la structure
                role_tokens = len(role.split()) + 5
                
                total_tokens += content_tokens + role_tokens
            
            # Ajouter un surco√ªt pour la structure globale
            total_tokens += len(chat_history) * 3
            
            return total_tokens
        except Exception as e:
            self.logger.warning(f"Error counting tokens: {e}")
            return len(str(chat_history)) // 4  # Approximation grossi√®re
    
    def _estimate_tokens(self, text: str) -> int:
        """Estime le nombre de tokens dans un texte donn√©."""
        if not text:
            return 0
        
        # Approximation bas√©e sur l'analyse des patterns de tokenization GPT/Claude
        words = text.split()
        word_count = len(words)
        
        # Compter les caract√®res de ponctuation
        punctuation_count = len(re.findall(r'[.,!?;:()\[\]{}"\'`\-‚Äì‚Äî‚Ä¶]', text))
        
        # Compter les nombres
        number_sequences = re.findall(r'\d+', text)
        number_tokens = sum(len(num) // 3 + 1 for num in number_sequences)
        
        # Compter les retours √† la ligne
        newline_count = text.count('\n')
        
        # G√©rer les mots longs
        long_words = [w for w in words if len(w) > 10]
        extra_tokens_long_words = sum((len(w) - 5) // 5 for w in long_words)
        
        # Calcul final
        estimated_tokens = (
            word_count + 
            punctuation_count // 2 + 
            number_tokens +
            newline_count +
            extra_tokens_long_words
        )
        
        # Ajuster selon le ratio observ√© (~1.3 tokens par mot en moyenne)
        return int(estimated_tokens * 1.3)
    
    def _safe_getint(self, config: ConfigParser, section: str, option: str, default):
        """
        R√©cup√®re une valeur enti√®re de la configuration de mani√®re s√ªre.
        G√®re les cas o√π la valeur contient des commentaires inline.
        
        Args:
            default: Valeur par d√©faut (peut √™tre None ou un entier)
        
        Returns:
            int ou None selon la configuration et le d√©faut
        """
        try:
            # Si l'option n'existe pas et que le d√©faut est None, retourner None
            if not config.has_option(section, option):
                return default
                
            value = config.get(section, option)
            # Nettoyer la valeur en enlevant les commentaires potentiels
            if '#' in value:
                value = value.split('#')[0].strip()
            return int(value)
        except (ValueError, TypeError):
            if default is not None:
                self.logger.warning(f"Impossible de parser {section}.{option}='{value}' comme entier, utilisation de la valeur par d√©faut {default}")
            return default
    
    def _clean_history_for_titling(self, chat_history: List[Dict[str, str]]) -> List[Dict[str, str]]:
        """
        Nettoie l'historique en supprimant les blocs de code pour une analyse s√©mantique pure.
        
        Patterns √† supprimer:
        - Blocs de code markdown (```)
        - Blocs de diff (---, +++, @@)
        - Longues cha√Ænes de caract√®res techniques
        
        Args:
            chat_history: L'historique de conversation original
            
        Returns:
            L'historique nettoy√© pour l'analyse s√©mantique
        """
        cleaned_history = []
        
        for message in chat_history:
            cleaned_content = message['content']
            
            # Supprimer les blocs de code markdown (avec ou sans langage sp√©cifi√©)
            cleaned_content = re.sub(r'```[\s\S]*?```', '[CODE BLOCK REMOVED]', cleaned_content)
            
            # Supprimer les blocs de code inline
            cleaned_content = re.sub(r'`[^`]+`', '[CODE]', cleaned_content)
            
            # Supprimer les blocs de diff
            # Pattern pour les lignes de diff qui commencent par ---, +++, @@, +, -
            cleaned_content = re.sub(r'^[\+\-@]{1,3}.*$', '[DIFF LINE REMOVED]', cleaned_content, flags=re.MULTILINE)
            
            # Remplacer les multiples espaces/lignes vides par un seul espace
            cleaned_content = re.sub(r'\n{3,}', '\n\n', cleaned_content)
            cleaned_content = re.sub(r'[ \t]{2,}', ' ', cleaned_content)
            
            # Ne pas tronquer ici - la troncature sera faite lors du formatage final
            
            cleaned_history.append({
                'role': message['role'],
                'content': cleaned_content.strip()
            })
        
        return cleaned_history
    
    def _get_title_generation_config(self) -> Dict[str, Any]:
        """
        R√©cup√®re la configuration pour la g√©n√©ration de titre avec fallback.
        Priorit√© : TitleGeneratorLLM > Mod√®le LLM par d√©faut
        
        Returns:
            Dict contenant la configuration pour g√©n√©rer le titre
        """
        try:
            # Lire le fichier config.ini
            config = ConfigParser()
            config.read('config.ini', encoding='utf-8')
            
            # Tenter d'abord la config sp√©cifique TitleGeneratorLLM
            if config.has_section('TitleGeneratorLLM'):
                if config.getboolean('TitleGeneratorLLM', 'enabled', fallback=True):
                    self.logger.info("Utilisation de la configuration TitleGeneratorLLM")
                    
                    # Si TitleGeneratorLLM n'a pas d'URL/apikey, utiliser le mod√®le par d√©faut
                    default_model = self._llm_models.get(self._default_llm_id, {}) if self._default_llm_id else {}
                    
                    # R√©cup√©rer les param√®tres avec fallback sur le mod√®le par d√©faut
                    return {
                        'api_url': config.get('TitleGeneratorLLM', 'url', 
                                            fallback=default_model.get('url', '')),
                        'api_key': config.get('TitleGeneratorLLM', 'apikey', 
                                             fallback=default_model.get('apikey', '')),
                        'model': config.get('TitleGeneratorLLM', 'model', 
                                          fallback=default_model.get('model', '')),
                        'api_type': config.get('TitleGeneratorLLM', 'api_type',
                                             fallback=default_model.get('api_type', 'openai')),
                        'prompt': config.get('TitleGeneratorLLM', 'title_prompt', 
                                           fallback=self.DEFAULT_TITLE_PROMPT),
                        'timeout': self._safe_getint(config, 'TitleGeneratorLLM', 'timeout_seconds', 15),
                        'max_length': self._safe_getint(config, 'TitleGeneratorLLM', 'max_title_length', 100),
                        'temperature': config.getfloat('TitleGeneratorLLM', 'temperature', fallback=None),
                        'max_tokens': self._safe_getint(config, 'TitleGeneratorLLM', 'max_tokens', None),
                        'ssl_verify': default_model.get('ssl_verify', True),
                        # Configuration proxy
                        'proxy_http': config.get('TitleGeneratorLLM', 'proxy_http', 
                                                fallback=default_model.get('proxy_http')),
                        'proxy_https': config.get('TitleGeneratorLLM', 'proxy_https', 
                                                 fallback=default_model.get('proxy_https')),
                        'proxy_no_proxy': config.get('TitleGeneratorLLM', 'proxy_no_proxy', 
                                                    fallback=default_model.get('proxy_no_proxy'))
                    }
                else:
                    self.logger.info("TitleGeneratorLLM est d√©sactiv√©, fallback sur le mod√®le par d√©faut")
            
            # Fallback sur le mod√®le LLM par d√©faut
            if self._default_llm_id and self._default_llm_id in self._llm_models:
                default_model = self._llm_models[self._default_llm_id]
                self.logger.info(f"Utilisation du mod√®le par d√©faut '{self._default_llm_id}' pour la g√©n√©ration de titre")
                return {
                    'api_url': default_model.get('url', ''),
                    'api_key': default_model.get('apikey', ''),
                    'model': default_model.get('model', ''),
                    'api_type': default_model.get('api_type', 'openai'),
                    'prompt': self.DEFAULT_TITLE_PROMPT,
                    'timeout': 15,
                    'max_length': 100,
                    'temperature': None,  # Pas de temperature par d√©faut
                    'max_tokens': None,  # Pas de max_tokens par d√©faut
                    'ssl_verify': default_model.get('ssl_verify', True),
                    # Configuration proxy
                    'proxy_http': default_model.get('proxy_http'),
                    'proxy_https': default_model.get('proxy_https'),
                    'proxy_no_proxy': default_model.get('proxy_no_proxy')
                }
            
            # Aucune configuration trouv√©e
            self.logger.warning("Aucune configuration LLM trouv√©e pour la g√©n√©ration de titre")
            return None
            
        except Exception as e:
            self.logger.error(f"Erreur lors de la lecture de la configuration: {e}")
            return None
    
    def extract_project_name(self, main_context: str) -> str:
        """
        Extrait le nom du projet depuis le contexte principal.
        
        Recherche les patterns suivants (par ordre de priorit√©):
        1. "# Contexte du projet - {nom}"
        2. "# Project Context - {nom}"
        3. Premier titre de niveau 1 ou 2
        
        Args:
            main_context: Le contexte complet du projet
            
        Returns:
            Le nom du projet ou une cha√Æne vide
        """
        if not main_context:
            return ""
        
        # Pattern 1: Format fran√ßais
        match = re.search(r'#\s*Contexte du projet\s*[-‚Äì]\s*(.+?)(?:\n|$)', main_context, re.IGNORECASE)
        if match:
            return match.group(1).strip()
        
        # Pattern 2: Format anglais
        match = re.search(r'#\s*Project Context\s*[-‚Äì]\s*(.+?)(?:\n|$)', main_context, re.IGNORECASE)
        if match:
            return match.group(1).strip()
        
        # Pattern 3: Premier titre de niveau 1 ou 2
        match = re.search(r'^#{1,2}\s+(.+?)(?:\n|$)', main_context, re.MULTILINE)
        if match:
            # Nettoyer le titre (enlever les parties apr√®s - ou :)
            title = match.group(1).strip()
            # Si le titre contient un s√©parateur, prendre la partie apr√®s
            if ' - ' in title:
                title = title.split(' - ', 1)[1]
            elif ': ' in title:
                title = title.split(': ', 1)[1]
            return title.strip()
        
        return ""
    
    def generate_title(self, chat_history: List[Dict[str, str]], main_context: Optional[str] = None) -> str:
        """
        G√©n√®re un titre pour une conversation en utilisant le LLM.
        
        Args:
            chat_history: L'historique de la conversation √† r√©sumer.
            main_context: Le contexte complet du projet pour en extraire le nom.
            
        Returns:
            Le titre sugg√©r√© par le LLM pr√©fix√© par le nom du projet, ou une cha√Æne vide en cas d'√©chec.
        """
        try:
            # Extraire le nom du projet
            project_name = self.extract_project_name(main_context)
            if project_name:
                self.logger.info(f"Nom du projet extrait: '{project_name}'")
            
            # R√©cup√©rer la configuration avec fallback
            title_config = self._get_title_generation_config()
            
            if not title_config:
                self.logger.info("Pas de configuration LLM disponible pour g√©n√©rer un titre")
                return ""
            
            # Nettoyer l'historique pour l'analyse s√©mantique
            cleaned_history = self._clean_history_for_titling(chat_history)
            
            # Limiter le nombre de messages pour √©conomiser les tokens
            if len(cleaned_history) > 10:
                # Prendre les 5 premiers et 5 derniers messages
                cleaned_history = cleaned_history[:5] + cleaned_history[-5:]
            
            # Formater l'historique pour l'inclure dans le prompt
            # Tronquer chaque message √† 300 caract√®res pour √©conomiser les tokens
            formatted_history = "\n".join([
                f"- {msg['role'].capitalize()}: {msg['content'][:300]}..." if len(msg['content']) > 300 
                else f"- {msg['role'].capitalize()}: {msg['content']}"
                for msg in cleaned_history
            ])
            
            # Pr√©parer le prompt final
            final_prompt = title_config['prompt'].format(history=formatted_history)
            
            # Cr√©er un message unique pour l'appel LLM
            title_request_history = [{'role': 'user', 'content': final_prompt}]
            
            # Pr√©parer la requ√™te selon le type d'API
            headers = {"Content-Type": "application/json"}
            if title_config['api_key']:
                headers["Authorization"] = f"Bearer {title_config['api_key']}"
            
            # Log pour debug
            self.logger.info(f"Configuration de g√©n√©ration de titre - api_type: {title_config.get('api_type', 'NON D√âFINI')}")
            
            if title_config['api_type'] == "openai":
                # Utiliser la m√©thode factoris√©e
                target_url, payload = self._build_openai_request(
                    api_url=title_config['api_url'],
                    model=title_config['model'],
                    messages=title_request_history,
                    stream=False,
                    temperature=title_config.get('temperature'),
                    max_tokens=title_config.get('max_tokens')
                )
            else:  # ollama
                prompt = title_request_history[0]['content']
                payload = {
                    "model": title_config['model'],
                    "prompt": prompt,
                    "stream": False
                }
                target_url = title_config['api_url'].rstrip('/') + "/api/generate"
            
            # Faire l'appel API avec timeout court
            self.logger.info(f"G√©n√©ration du titre via {target_url} (api_type={title_config['api_type']})")
            
            if not title_config['ssl_verify']:
                import urllib3
                urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
            
            # R√©cup√©rer la config proxy depuis title_config ou fallback sur le mod√®le par d√©faut
            proxy_config = {
                'proxy_http': title_config.get('proxy_http'),
                'proxy_https': title_config.get('proxy_https'),
                'proxy_no_proxy': title_config.get('proxy_no_proxy')
            }
            proxies = self._get_proxy_config(proxy_config) if any(proxy_config.values()) else {}
            
            response = self.session.post(
                target_url,
                headers=headers,
                json=payload,
                verify=title_config['ssl_verify'],
                timeout=title_config['timeout'],
                proxies=proxies
            )
            
            response.raise_for_status()
            
            # Parser la r√©ponse selon le type d'API
            if title_config['api_type'] == "openai":
                result = response.json()
                
                if 'choices' in result and result['choices']:
                    # R√©cup√©rer le message correctement
                    choice = result['choices'][0]
                    
                    # V√©rifier si la g√©n√©ration a √©t√© tronqu√©e
                    if choice.get('finish_reason') == 'length':
                        self.logger.warning("La g√©n√©ration a √©t√© tronqu√©e (finish_reason='length'). Le LLM a besoin de plus de tokens.")
                    
                    if 'message' in choice:
                        message = choice['message']
                        title = message.get('content')
                    elif 'text' in choice:  # Format legacy OpenAI
                        title = choice.get('text')
                    else:
                        self.logger.warning(f"Structure de choice inattendue. Cl√©s disponibles: {choice.keys() if isinstance(choice, dict) else type(choice)}")
                        title = None
                else:
                    self.logger.warning(f"Format de r√©ponse inattendu du LLM - pas de choices: {result.keys()}")
                    return ""
            else:  # ollama
                result = response.json()
                self.logger.debug(f"R√©ponse brute du LLM (Ollama): {result}")
                if 'response' in result:
                    title = result.get('response')
                else:
                    self.logger.warning(f"Format de r√©ponse inattendu du LLM: {result.keys()}")
                    return ""
            
            # V√©rifier que le titre n'est pas None
            if not title:
                self.logger.warning(f"Le LLM a retourn√© un titre vide ou None. Result keys: {result.keys() if result else 'No result'}")
                return ""
            
            # Nettoyer le titre
            # Supprimer les guillemets, apostrophes au d√©but et √† la fin
            title = str(title).strip().strip('"\'`')
            
            # Supprimer les pr√©fixes courants
            prefixes_to_remove = ['Titre :', 'Title:', 'Titre:', 'Title :', 
                                 'Sujet :', 'Subject:', 'R√©sum√© :', 'Summary:']
            for prefix in prefixes_to_remove:
                if title.lower().startswith(prefix.lower()):
                    title = title[len(prefix):].strip()
            
            # Construire le titre final avec le nom du projet
            if project_name and title:
                # Format: "NomProjet: Titre"
                final_title = f"{project_name}: {title}"
                self.logger.info(f"Titre avec pr√©fixe projet: '{final_title}'")
            else:
                final_title = title
            
            # V√©rifier la longueur et tronquer si n√©cessaire
            max_length = title_config['max_length']
            if len(final_title) > max_length:
                # Tronquer intelligemment en gardant le nom du projet
                if project_name and len(project_name) < max_length - 10:
                    # Garder le nom du projet et tronquer le titre
                    remaining = max_length - len(project_name) - 2  # -2 pour ": "
                    truncated_title = title[:remaining-3] + "..."
                    final_title = f"{project_name}: {truncated_title}"
                else:
                    # Tronquer normalement
                    final_title = final_title[:max_length-3] + "..."
            
            self.logger.info(f"Titre final g√©n√©r√©: '{final_title}'")
            return final_title
            
        except requests.exceptions.Timeout:
            timeout_val = title_config.get('timeout', 15) if title_config else 15
            self.logger.warning(f"Timeout lors de la g√©n√©ration du titre (>{timeout_val}s)")
            return ""
        except requests.exceptions.RequestException as e:
            self.logger.error(f"Erreur r√©seau lors de la g√©n√©ration du titre: {e}")
            return ""
        except Exception as e:
            self.logger.error(f"Erreur lors de la g√©n√©ration du titre: {e}")
            return ""