[Debug]
# Active le mode debug pour pywebview et les logs détaillés
debug = false

[Instructions]
instruction1_text = Ne fais rien, attends mes instructions.
instruction2_text = Si des modifications du code source est nécessaire, tu dois présenter ta réponse sous la forme d'un fichier patch Linux. Considère que le fichier patch a été lancé depuis le répertoire du code-to-llm.

# Configuration multi-modèles LLM
# Chaque section [LLM:nom] définit un modèle complètement indépendant
# Le nom après "LLM:" sera affiché dans le sélecteur de l'interface

[LLM:GPT-4o]
url = https://api.openai.com/v1
apikey = YOUR_OPENAI_API_KEY_HERE
model = gpt-4o
api_type = openai
enabled = false
stream_response = true
ssl_verify = true
timeout_seconds = 300
# Contrôle la créativité (0.0 = déterministe, 1.0 = très créatif)
temperature = 0.7
# Nombre maximum de tokens pour la réponse
# Commentez cette ligne pour laisser le modèle utiliser son maximum par défaut
# Pour Gemini 2.0 Flash : max 8192 tokens en output
# max_tokens = 8192
# Ce modèle sera sélectionné par défaut au démarrage
default = true

[LLM:Claude 3.5 Sonnet]
url = https://api.anthropic.com/v1
apikey = YOUR_ANTHROPIC_API_KEY_HERE
model = claude-3-5-sonnet-20241022
# ou 'openai' si utilisation via proxy OpenAI-compatible
api_type = anthropic
enabled = false
stream_response = true
ssl_verify = true
timeout_seconds = 300
temperature = 0.5
# Claude peut générer jusqu'à 8192 tokens
# max_tokens = 8192
default = false

[LLM:Ollama Local]
url = http://localhost:11434
# Pas de clé API pour Ollama local
apikey =
model = llama3:70b
api_type = ollama
enabled = false
# Peut être désactivé pour certains modèles
stream_response = false
# Pas de SSL pour une connexion locale
ssl_verify = false
# Timeout plus long pour les modèles locaux
timeout_seconds = 600
temperature = 0.8
# Laissez commenté pour utiliser le maximum du modèle
# max_tokens = 4096

# Ancienne configuration (conservée pour compatibilité, sera ignorée si des sections [LLM:*] existent)
# [LLMServer]
# url = YOUR_LLM_API_URL_HERE
# apikey = YOUR_LLM_API_KEY_HERE
# model = YOUR_LLM_MODEL_HERE
# api_type = openai
# enabled = false
# stream_response = true
# ssl_verify = true
# timeout_seconds = 300

[SummarizerLLM]
# LLM pour le RESUME du code (compression "lossy")
# Peut être différent du LLM de chat. Ex: un modèle spécialisé en code.
url = http://localhost:11434
apikey = YOUR_LLM_API_KEY_HERE
model = llama3:70b
api_type = ollama
enabled = true
summarizer_prompt = Tu es un expert en analyse de code source. Analyse le fichier `{file_path}` et fournis un résumé concis au format JSON. Le format de sortie doit être EXCLUSIVEMENT un objet JSON valide.\n\nInstructions pour chaque clé :\n- "role": Décris en une phrase le rôle principal du fichier (ex: "Serveur web Flask pour l'application principale", "Logique frontend pour l'interaction utilisateur", "Module de construction de contexte LLM").\n- "public_interface": Liste les fonctions, classes, ou endpoints API principaux qui sont destinés à être utilisés par d'autres parties du code. Sois concis. Pour du HTML, liste les sections principales. Pour du CSS, les classes majeures.\n- "dependencies": Liste les modules ou fichiers importés qui sont essentiels à ce fichier.\n\nCode du fichier `{file_path}`:\n---\n{content}\n---
summarizer_timeout_seconds = 300
summarizer_max_workers = 2

[TitleGeneratorLLM]
# Configuration optionnelle pour la génération automatique de titres de conversation
# Si cette section est absente ou disabled, le système utilisera [LLMServer] en fallback
enabled = true
# URL et clé API spécifiques (optionnelles, fallback sur LLMServer si non définies)
# url = YOUR_TITLE_LLM_API_URL_HERE
# apikey = YOUR_TITLE_LLM_API_KEY_HERE
# model = gpt-3.5-turbo  # Modèle plus léger pour la génération de titres
  # Hérite de LLMServer si non défini:
api_type = openai
title_prompt = En te basant sur l'historique de conversation suivant, génère un titre court et descriptif (maximum 10 mots) qui résume le sujet principal. Réponds UNIQUEMENT avec le titre, sans guillemets ni préfixe comme "Titre :".\n\n### Historique\n{history}
# Longueur maximale du titre en caractères
max_title_length = 100
# Timeout court pour ne pas bloquer l'UX
timeout_seconds = 15
# Temperature pour la créativité (0.0 = déterministe, 1.0 = créatif)
# temperature = 0.7
# Nombre maximum de tokens pour la réponse (optionnel, peut causer des problèmes avec certaines APIs)
# max_tokens = 100


[Tokens]
# Optional: API key for token calculation services if you use one


[BinaryDetection]
# Fichiers immédiatement rejetés (séparés par des virgules)
extension_blacklist = .png, .jpg, .jpeg, .gif, .bmp, .ico, .exe, .dll, .so, .pdf, .zip, .gz, .rar, .mp3, .mp4, .avi, .mov, .woff, .woff2, .eot, .ttf, .otf, .class, .jar, .pyc, .bin, .dat, .dmg, .iso, .msi, .cjs

# Fichiers immédiatement acceptés sans analyse de contenu (séparés par des virgules)
extension_whitelist = .py, .js, .html, .css, .json, .md, .txt, .sh, .yml, .yaml, .xml, .ini, .cfg, .conf, .rst, .ts, .jsx, .tsx, .toml, .sql

[FileExclusion]
# Fichiers spécifiques à exclure (séparés par des virgules)
# Ces fichiers sont utiles dans Git mais pas pour le contexte LLM
file_blacklist = .pnp.cjs, .pnp.loader.mjs, package-lock.json, yarn.lock, .DS_Store, Thumbs.db

# Patterns à exclure (supporte * et ?)
# Exemples: *.min.js, *-lock.json, test-*.js
pattern_blacklist = *.min.js, *.min.css, *-lock.json, *.map
