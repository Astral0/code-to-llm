# Plan d'action : Refactoring de la configuration multi-LLM

Bonjour ! üëã

Voici le guide d√©taill√© pour le refactoring de la configuration des LLMs. L'objectif est de rendre chaque mod√®le d√©fini dans `config.ini` totalement autonome et de permettre √† l'utilisateur de basculer entre eux directement depuis l'interface.

Suis ces √©tapes attentivement. Le plan est con√ßu pour √™tre suivi de mani√®re s√©quentielle. N'h√©site jamais √† poser des questions si tu es bloqu√©.

## √âtat actuel du code
Apr√®s analyse du codebase, voici les constats :
- La configuration actuelle utilise une section `[LLMServer]` unique dans `config.ini.template`
- Il existe des sections sp√©cifiques pour `[SummarizerLLM]` et `[TitleGeneratorLLM]` qui h√©ritent de `[LLMServer]`
- Le code ne supporte pas encore la s√©lection dynamique entre plusieurs mod√®les LLM
- `main_desktop.py` charge la configuration LLM dans `load_service_configs()` (lignes 135-177)
- `LlmApiService` utilise directement la configuration pass√©e dans son constructeur

## Phase 0 : Pr√©paration et Installation

Avant toute modification, assurons-nous que notre base de travail est saine et stable.

- [ ] **T√¢che :** Cr√©er une nouvelle branche Git pour ce chantier.
  - Cela nous permet de travailler en toute s√©curit√© sans impacter le code principal.
  - Ex√©cute ces commandes :
    ```bash
    git checkout main
    git pull
    git checkout -b feature/multi-llm-config
    ```
  - ‚úÖ **Crit√®re d'acceptation :** La commande `git status` confirme que tu es sur la branche `feature/multi-llm-config`.

- [ ] **T√¢che :** Valider l'environnement de d√©veloppement.
  - Suis les instructions du `README.md` pour activer l'environnement Conda (`conda activate code2llm`) et lancer l'application avec `run.bat`.
  - ‚úÖ **Crit√®re d'acceptation :** La fen√™tre principale de l'application s'ouvre sans aucune erreur dans la console.

- [ ] **T√¢che :** Lancer la suite de tests pour √©tablir une r√©f√©rence.
  - C'est notre filet de s√©curit√©. Nous devons nous assurer que tout est vert avant de commencer.
  - Dans le terminal, avec l'environnement activ√©, ex√©cute :
    ```bash
    pytest
    ```
  - ‚úÖ **Crit√®re d'acceptation :** La commande `pytest` se termine avec succ√®s, tous les tests sont marqu√©s comme "passed".

## Phase 1 : Refactoring du Backend

Objectif : Adapter le code pour qu'il comprenne la nouvelle structure de configuration autonome.

- [ ] **T√¢che :** Mettre √† jour la lecture de la configuration dans `main_desktop.py`.
  - Nous allons modifier `load_service_configs()` pour qu'elle lise chaque section `[LLM:...]` comme une entit√© compl√®te et ind√©pendante.
  - Dans `main_desktop.py`, localise `load_service_configs()` et remplace la logique de configuration LLM par celle-ci :
    ```python
    # Dans main_desktop.py -> load_service_configs()
    
    llm_models = {}
    default_llm_id = None
    for section in config.sections():
        if section.startswith('LLM:'):
            # Ignorer les mod√®les d√©sactiv√©s
            if not config.getboolean(section, 'enabled', fallback=True):
                continue
    
            llm_id = section[4:].strip()
            is_default = config.getboolean(section, 'default', fallback=False)
    
            llm_models[llm_id] = {
                'id': llm_id,
                'name': llm_id,
                'url': config.get(section, 'url', fallback=''),
                'apikey': config.get(section, 'apikey', fallback=''),
                'model': config.get(section, 'model', fallback=''),
                'api_type': config.get(section, 'api_type', fallback='openai').lower(),
                'stream_response': config.getboolean(section, 'stream_response', fallback=False),
                'ssl_verify': config.getboolean(section, 'ssl_verify', fallback=True),
                'timeout_seconds': config.getint(section, 'timeout_seconds', fallback=300),
                'temperature': safe_parse_config_value(config, section, 'temperature', float, None),
                'max_tokens': safe_parse_config_value(config, section, 'max_tokens', int, None),
                'default': is_default
            }
            if is_default:
                default_llm_id = llm_id
    
    service_configs['llm_service'] = {
        'models': llm_models,
        'default_id': default_llm_id
    }
    ```
  - ‚úÖ **Crit√®re d'acceptation :** L'application se lance toujours sans erreur.

- [ ] **T√¢che :** Adapter le service `LlmApiService` √† la nouvelle structure.
  - Le constructeur du service doit √™tre mis √† jour pour accepter la nouvelle structure de configuration.
  - Dans `services/llm_api_service.py`, modifie la m√©thode `__init__` :
    ```python
    # Dans services/llm_api_service.py -> __init__
    super().__init__(config, logger)
    self._llm_models = config.get('models', {})
    self._default_llm_id = config.get('default_id', None)
    self._setup_http_session()
    ```
  - Ensuite, ajoute une m√©thode pour obtenir les mod√®les disponibles :
    ```python
    def get_available_models(self):
        """Retourne la liste des mod√®les LLM disponibles."""
        return [
            {'id': llm_id, 'name': model['name'], 'default': model.get('default', False)}
            for llm_id, model in self._llm_models.items()
        ]
    ```
  - Modifie la m√©thode `_prepare_request` pour qu'elle accepte un param√®tre `llm_id` et utilise la configuration du mod√®le demand√© :
    ```python
    # Dans services/llm_api_service.py -> _prepare_request
    def _prepare_request(self, chat_history: List[Dict[str, str]], stream: bool = False, llm_id: Optional[str] = None) -> tuple:
        target_llm_id = llm_id if llm_id and llm_id in self._llm_models else self._default_llm_id
        if not target_llm_id:
            raise LlmApiServiceException('No default or valid LLM configured.')
        
        final_config = self._llm_models[target_llm_id]
        # ... le reste de la fonction utilise `final_config` pour tous les param√®tres.
    ```
  - ‚úÖ **Crit√®re d'acceptation :** Adapte les tests unitaires dans `tests/test_llm_api_service.py` pour qu'ils passent avec la nouvelle logique.

- [ ] **T√¢che :** Mettre √† jour les m√©thodes de l'API dans `main_desktop.py`.
  - L'API doit √™tre "stateless". Nous exposons la liste des mod√®les et nous nous attendons √† recevoir l'ID du mod√®le √† chaque appel.
  - Dans la classe `Api` de `main_desktop.py` :
    1. Ajoute une nouvelle m√©thode : `get_available_llms()`.
    2. Modifie les signatures de `send_to_llm` et `send_to_llm_stream` pour qu'elles acceptent un param√®tre `llm_id`.
    ```python
    # Dans main_desktop.py -> class Api
    
    def get_available_llms(self):
        """Retourne la liste des LLMs configur√©s."""
        return self.llm_service.get_available_models()
    
    def send_to_llm(self, chat_history, stream=False, llm_id=None):
        # ... passe llm_id √† self.llm_service.send_to_llm
        return self.llm_service.send_to_llm(chat_history, stream, llm_id)
    
    def send_to_llm_stream(self, chat_history, callback_id, llm_id=None):
        # ... passe llm_id dans le thread
        def stream_worker():
            # ... appel avec llm_id
    ```
  - ‚úÖ **Crit√®re d'acceptation :** Les nouvelles m√©thodes sont en place et les signatures sont mises √† jour.

## Phase 2 : Mise √† jour du Frontend

Objectif : Cr√©er l'interface permettant √† l'utilisateur de choisir son mod√®le.

- [ ] **T√¢che :** Ajouter le s√©lecteur de LLM dans `templates/toolbox.html`.
  - Nous allons ajouter un menu d√©roulant (`<select>`) dans la zone de contr√¥les du chat.
  - Dans `templates/toolbox.html`, juste avant la barre de contr√¥le de navigation, ajoute ce bloc :
    ```html
    <!-- S√©lecteur de LLM -->
    <div class="d-flex align-items-center api-mode-only">
        <label for="llmSelector" class="form-label-sm me-2 mb-0 text-muted">Mod√®le:</label>
        <select class="form-select form-select-sm" id="llmSelector" style="width: auto;" title="Choisir le mod√®le LLM √† utiliser">
            <!-- Options charg√©es dynamiquement -->
        </select>
    </div>
    ```
  - ‚úÖ **Crit√®re d'acceptation :** Un menu d√©roulant vide appara√Æt dans l'interface de la Toolbox.

- [ ] **T√¢che :** Impl√©menter la logique frontend dans `static/toolbox.js`.
  - Le JavaScript doit charger la liste des mod√®les, la persister et l'envoyer au backend √† chaque requ√™te.
  - Dans `static/toolbox.js` :
    1. Cr√©e une nouvelle fonction `loadAvailableLlmModels()` qui appelle `window.pywebview.api.get_available_llms()` et remplit le s√©lecteur. Appelle cette fonction dans `initializeUI()`.
    2. Sauvegarde le choix de l'utilisateur dans le `localStorage` √† chaque changement du s√©lecteur.
    3. Au chargement, essaie de restaurer le choix depuis le `localStorage`.
    4. Dans `sendMessage` et `sendMessageStream`, r√©cup√®re la valeur de `#llmSelector` et passe-la dans l'appel √† l'API backend.
  - ‚úÖ **Crit√®re d'acceptation :** Le s√©lecteur se remplit avec les mod√®les du `config.ini`. Le choix est conserv√© apr√®s un rechargement. L'envoi d'un message fonctionne.

## Phase 3 : Configuration et Documentation

Objectif : Rendre la nouvelle configuration facile √† comprendre et √† utiliser.

- [ ] **T√¢che :** Mettre √† jour `config.ini.template`.
  - Le template doit servir d'exemple clair pour la nouvelle structure.
  - Supprime compl√®tement la section `[LLMServer]`.
  - Assure-toi que chaque section `[LLM:...]` contient tous les param√®tres n√©cessaires (`model`, `api_type`, `enabled`, etc.).
  - Ajoute des commentaires pour expliquer les nouveaux param√®tres `enabled = true/false` et `default = true`.
  - ‚úÖ **Crit√®re d'acceptation :** Le fichier `config.ini.template` est propre, √† jour et bien comment√©.

- [ ] **T√¢che :** Mettre √† jour le `README.md`.
  - La documentation est essentielle pour que les utilisateurs comprennent ce changement majeur.
  - Dans la section "Configuration Essentielle" du `README.md`, remplace l'ancien exemple de configuration par le nouveau, en montrant un exemple avec plusieurs mod√®les.
  - ‚úÖ **Crit√®re d'acceptation :** Le `README.md` explique clairement comment configurer plusieurs LLMs.

- [ ] **T√¢che (optionnelle) :** Cr√©er un script de migration `migrate_config.py`.
  - Pour faciliter la transition des utilisateurs existants.
  - Le script doit lire l'ancien format avec `[LLMServer]` et g√©n√©rer le nouveau format.
  - Faire une sauvegarde de l'ancienne configuration avant modification.
  - ‚úÖ **Crit√®re d'acceptation :** Les utilisateurs peuvent migrer leur configuration existante automatiquement.

## Phase 4 : Tests et Validation

Objectif : Garantir que le refactoring n'a introduit aucune r√©gression et que la nouvelle fonctionnalit√© est robuste.

- [ ] **T√¢che :** Adapter les tests unitaires et d'int√©gration.
  - Les tests doivent refl√©ter l'architecture stateless.
  - Dans `tests/test_llm_api_service.py` et `tests/test_api_integration.py`, modifie les tests pour passer l'`llm_id` aux fonctions `send_...` et v√©rifie que la bonne configuration est utilis√©e.
  - Ajoute un test qui v√©rifie qu'un mod√®le avec `enabled = false` n'appara√Æt pas dans la liste retourn√©e par `get_available_llms()`.
  - ‚úÖ **Crit√®re d'acceptation :** Tous les tests dans la suite `pytest` passent avec succ√®s.

- [ ] **T√¢che :** Effectuer des tests manuels complets.
  - Cr√©e un `config.ini` avec au moins 3 mod√®les (ex: un Ollama, un OpenAI, et un d√©sactiv√©).
  - V√©rifie que seuls les mod√®les activ√©s apparaissent dans le s√©lecteur.
  - V√©rifie que le mod√®le par d√©faut est bien s√©lectionn√© au d√©marrage.
  - Teste l'envoi de messages avec chaque mod√®le et valide que les r√©ponses proviennent bien des bons services.
  - Teste les diff√©rents param√®tres (ex: `stream_response = false` sur un mod√®le) pour confirmer qu'ils sont bien respect√©s.
  - ‚úÖ **Crit√®re d'acceptation :** Tous les sc√©narios de test manuels se d√©roulent comme attendu.

## Phase 5 : Finalisation

La derni√®re ligne droite !

- [ ] **T√¢che :** Cr√©er une Pull Request (PR) claire et concise.
  - Assure-toi que tout ton code est "commit" et pouss√© sur ta branche.
  - Ouvre une PR vers la branche principale.
  - Dans la description, r√©sume les changements, explique le "breaking change" pour le `config.ini` et fais r√©f√©rence √† ce plan d'action.
  - ‚úÖ **Crit√®re d'acceptation :** La Pull Request est cr√©√©e et pr√™te pour la relecture par l'√©quipe.

## Points d'attention importants

### ‚ö†Ô∏è Architecture stateless
- **Important :** L'API backend doit rester stateless. Chaque appel doit inclure l'ID du mod√®le √† utiliser.
- Le frontend g√®re la persistance du choix utilisateur via le localStorage.
- Pas de notion de "mod√®le actif" c√¥t√© backend, uniquement un mod√®le par d√©faut.

### üîÑ Breaking changes
- Les utilisateurs devront migrer leur `config.ini` existant.
- La section `[LLMServer]` n'existera plus.
- Les int√©grations externes devront passer l'`llm_id` dans leurs appels.

### üîí S√©curit√©
- Les cl√©s API doivent rester isol√©es par mod√®le.
- Ne jamais exposer les cl√©s API dans les logs ou l'interface.
- Valider que l'`llm_id` re√ßu existe bien dans la configuration.

### üß™ Tests critiques √† ne pas oublier
1. Configuration avec 0 mod√®le ‚Üí Message d'erreur clair.
2. Configuration avec 1 seul mod√®le ‚Üí Le s√©lecteur peut √™tre masqu√© ou disabled.
3. Configuration avec N mod√®les ‚Üí S√©lecteur fonctionnel avec le bon d√©faut.
4. Mod√®le avec `enabled = false` ‚Üí N'appara√Æt pas dans la liste.
5. Changement de mod√®le en cours de conversation ‚Üí Continuit√© assur√©e.

## Structure de configuration cible

```ini
# Exemple de configuration multi-mod√®les

[LLM:GPT-4o]
url = https://api.openai.com/v1
apikey = sk-xxxxxxxxxxxxx
model = gpt-4o
api_type = openai
enabled = true  # Si false, ce mod√®le n'appara√Ætra pas dans le s√©lecteur
stream_response = true
ssl_verify = true
timeout_seconds = 300
temperature = 0.7
max_tokens = 4096
default = true  # Ce mod√®le sera s√©lectionn√© par d√©faut au d√©marrage

[LLM:Claude-3.5 Sonnet]
url = https://api.anthropic.com/v1
apikey = sk-ant-xxxxxxxxxxxxx
model = claude-3-5-sonnet-20241022
api_type = anthropic  # ou openai si utilisation via proxy OpenAI-compatible
enabled = true
stream_response = true
ssl_verify = true
timeout_seconds = 300
temperature = 0.5
max_tokens = 8192
default = false

[LLM:Ollama Llama3 Local]
url = http://localhost:11434
apikey =  # Pas de cl√© API pour Ollama local
model = llama3:70b
api_type = ollama
enabled = true
stream_response = false  # D√©sactiv√© pour ce mod√®le sp√©cifique
ssl_verify = false
timeout_seconds = 600  # Timeout plus long pour les mod√®les locaux
temperature = 0.8
# max_tokens non d√©fini : utilisera le d√©faut du mod√®le

[LLM:Mod√®le D√©sactiv√©]
url = https://api.example.com
apikey = xxx
model = test-model
api_type = openai
enabled = false  # Ce mod√®le n'appara√Ætra PAS dans le s√©lecteur

# Note: Les sections SummarizerLLM et TitleGeneratorLLM peuvent √™tre conserv√©es
# pour une future √©volution, mais ne sont pas trait√©es dans ce refactoring
```

## Crit√®res de succ√®s

‚úÖ **Le refactoring est r√©ussi si :**
1. L'application fonctionne avec plusieurs mod√®les LLM configur√©s ind√©pendamment.
2. L'utilisateur peut changer de mod√®le via le s√©lecteur dans l'interface.
3. Chaque appel API utilise la configuration compl√®te du mod√®le sp√©cifi√©.
4. Les mod√®les avec `enabled = false` n'apparaissent pas dans le s√©lecteur.
5. Le mod√®le marqu√© `default = true` est s√©lectionn√© au d√©marrage.
6. Le choix de l'utilisateur est persist√© dans le localStorage.
7. Tous les tests unitaires et d'int√©gration passent.
8. La documentation est claire sur le nouveau format de configuration.
9. Aucune r√©gression n'est introduite dans les fonctionnalit√©s existantes.

---

## Notes importantes pour l'impl√©mentation

### Philosophie de l'approche stateless
- Le backend ne maintient pas d'√©tat sur le mod√®le "actif".
- Chaque requ√™te contient l'ID du mod√®le √† utiliser.
- Si aucun ID n'est fourni, utiliser le mod√®le par d√©faut.
- Cela rend l'API plus flexible et facilite les int√©grations futures.

### Gestion des services sp√©cialis√©s (SummarizerLLM, TitleGeneratorLLM)
- **Pour cette premi√®re it√©ration**, nous ne touchons pas √† ces services.
- Ils continueront √† fonctionner avec leur logique actuelle.
- Une √©volution future pourra les int√©grer dans le syst√®me multi-mod√®les.

### Points de vigilance
1. **Validation des entr√©es** : Toujours v√©rifier que l'`llm_id` re√ßu existe.
2. **Fallback intelligent** : Si le mod√®le demand√© n'existe pas, utiliser le d√©faut.
3. **Gestion d'erreur** : Messages clairs si aucun mod√®le n'est configur√©.
4. **Compatibilit√© API** : L'API Anthropic peut n√©cessiter un format diff√©rent.

### Am√©liorations futures possibles
1. Indicateur visuel du mod√®le en cours d'utilisation dans les r√©ponses.
2. Possibilit√© de changer de mod√®le en cours de conversation.
3. Statistiques d'utilisation par mod√®le.
4. Tests automatiques de connexion pour chaque mod√®le configur√©.