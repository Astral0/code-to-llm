# üöÄ LLM Context Builder - √âdition Desktop

Une application de bureau compl√®te pour pr√©parer, analyser et interagir avec vos projets de code via des LLMs. Cr√©ez un contexte de projet s√©curis√© et optimis√©, et dialoguez avec une IA gr√¢ce √† une bo√Æte √† outils de d√©veloppement int√©gr√©e.

> üõ°Ô∏è **S√©curit√© avant tout** : Cet outil int√®gre un puissant m√©canisme de d√©tection et de masquage des secrets (cl√©s API, mots de passe, tokens) pour pr√©venir toute fuite d'informations sensibles vers les mod√®les de langage.

![Aper√ßu de l'interface](https://github.com/user-attachments/assets/e2816992-9967-403a-b8d6-3df6e618a0e)

## ‚ú® Fonctionnalit√©s Cl√©s

- **üñ•Ô∏è Application de Bureau Native** : Interface utilisateur web moderne (`Flask` + `pywebview`) encapsul√©e dans une application de bureau autonome pour une exp√©rience fluide et int√©gr√©e.
- **üß† Scan de Projet Intelligent** : Analyse les r√©pertoires locaux en respectant automatiquement les r√®gles `.gitignore` et en filtrant les fichiers non pertinents (binaires, logs, etc.).
- **üîê Masquage de Secrets Avanc√©** : Utilise `detect-secrets` et des expressions r√©guli√®res pour identifier et masquer les informations sensibles avant la g√©n√©ration du contexte.
- **üîÑ Persistance de S√©lection de Fichiers** : Sauvegarde automatiquement votre s√©lection de fichiers et permet de la restaurer en un clic lors de la prochaine ouverture du projet. Identifie et met en √©vidence les nouveaux fichiers ajout√©s depuis la derni√®re session.
- **üíæ Gestion de Conversations** : Sauvegardez, chargez, dupliquez et g√©rez vos sessions de chat avec **g√©n√©ration automatique de titre par IA**. Le syst√®me inclut le contexte du projet et un **m√©canisme de verrouillage** pour un travail multi-instances s√©curis√©.
- **üß∞ Toolbox D√©veloppeur Augment√©** : Un puissant assistant IA int√©gr√© avec deux modes :
    - **Mode API** : Un client de chat direct avec votre LLM configur√© (supporte OpenAI et Ollama), avec gestion de l'historique, streaming, et export des conversations.
    - **Mode Navigateur** : Pilote une fen√™tre de navigateur int√©gr√©e pour interagir avec des services comme ChatGPT, Gemini ou Claude AI directement depuis l'application.
- **üìö Biblioth√®que de Prompts** : Une collection de prompts pr√©d√©finis et personnalisables pour des t√¢ches complexes : analyse d'architecture, audit de s√©curit√©, planification de fonctionnalit√©s, etc.
- **üîÑ Int√©gration `git diff`** : Analysez en un clic les modifications en attente (`--staged`) pour g√©n√©rer des messages de commit ou obtenir des revues de code.
- **üìÑ Export Multi-format** : Exportez vos conversations au format Markdown, DOCX ou PDF.
- **‚ö° Gestion Avanc√©e des Erreurs** : Syst√®me de retry intelligent avec circuit breaker, failover automatique entre endpoints LLM et notifications visuelles en temps r√©el.
- **üåê Support Proxy d'Entreprise** : Configuration proxy HTTP/HTTPS pour acc√©der aux LLM externes depuis un environnement d'entreprise avec restrictions r√©seau.

## ‚öôÔ∏è Installation

### Pr√©requis
- Windows (l'application est optimis√©e pour Windows via `run.bat`)
- [Miniconda](https://docs.conda.io/en/latest/miniconda.html) ou Anaconda

### √âtapes d'installation
1.  Clonez le d√©p√¥t :
    ```bash
    git clone https://github.com/Astral0/code-to-llm.git
    cd code-to-llm
    ```
2.  Le script `run.bat` est con√ßu pour automatiser la configuration. Il va :
    - Chercher votre installation Conda.
    - V√©rifier et activer l'environnement `code2llm` (il doit exister).
    - Lancer l'application.

    Si c'est la premi√®re fois, cr√©ez l'environnement Conda :
    ```bash
    conda create -n code2llm python=3.9
    conda activate code2llm
    pip install -r requirements.txt
    ```
3.  **Configuration Essentielle** :
    Copiez `config.ini.template` vers `config.ini` et configurez-le. L'application supporte maintenant **plusieurs mod√®les LLM ind√©pendants** que vous pouvez s√©lectionner directement depuis l'interface :
    
    ```ini
    # Chaque section [LLM:nom] d√©finit un mod√®le ind√©pendant
    # Le nom apr√®s "LLM:" appara√Ætra dans le s√©lecteur de l'interface
    
    [LLM:GPT-4o]
    url = https://api.openai.com/v1
    apikey = YOUR_OPENAI_API_KEY_HERE
    model = gpt-4o
    api_type = openai
    enabled = true  # Mettre √† true pour activer ce mod√®le
    stream_response = true
    ssl_verify = true
    timeout_seconds = 300
    temperature = 0.7
    max_tokens = 4096
    default = true  # Ce mod√®le sera s√©lectionn√© par d√©faut
    # Configuration proxy optionnelle (d√©commentez si n√©cessaire)
    # proxy_http = http://proxy.entreprise.com:8080
    # proxy_https = http://proxy.entreprise.com:8080
    # proxy_no_proxy = localhost,127.0.0.1,.entreprise.local
    
    [LLM:Ollama Local]
    url = http://localhost:11434
    apikey =  # Pas de cl√© API pour Ollama local
    model = llama3:70b
    api_type = ollama
    enabled = true
    stream_response = false
    ssl_verify = false
    timeout_seconds = 600
    ```
    
    **Nouvelles fonctionnalit√©s :**
    - **Multi-mod√®les** : Configurez autant de mod√®les que n√©cessaire
    - **S√©lecteur dans l'interface** : Changez de mod√®le √† la vol√©e sans red√©marrer
    - **Configuration ind√©pendante** : Chaque mod√®le a ses propres param√®tres
    - **Persistance du choix** : Le mod√®le s√©lectionn√© est m√©moris√© entre les sessions
4.  Lancez l'application en double-cliquant sur **`run.bat`**.

## üöÄ Guide d'Utilisation

1.  **Scanner un Projet** :
    - Lancez l'application via `run.bat`.
    - Cliquez sur **"S√©lectionner un r√©pertoire"** pour ouvrir la bo√Æte de dialogue native.
    - Choisissez votre projet et cliquez sur **"Scanner le r√©pertoire"**.
2.  **S√©lectionner les Fichiers** :
    - L'arbre des fichiers de votre projet (filtr√©s) appara√Æt.
    - Si vous avez d√©j√† travaill√© sur ce projet, une section **"Session Pr√©c√©dente D√©tect√©e"** appara√Æt :
        - Cliquez sur le bouton **"Restaurer la s√©lection pr√©c√©dente"** pour r√©appliquer votre s√©lection de fichiers.
        - Les **nouveaux fichiers** ajout√©s depuis votre derni√®re session sont mis en √©vidence dans une section d√©di√©e.
    - Cochez les fichiers et dossiers que vous souhaitez inclure dans le contexte.
3.  **G√©n√©rer le Contexte** :
    - Dans la section 3, ajoutez des instructions initiales au LLM si n√©cessaire.
    - Choisissez un mode de **Compression** si besoin (Mode Compact ou R√©sum√© par IA).
    - Cliquez sur **"Generate context for selection"**.
4.  **Interagir avec l'IA** :
    - Le contexte Markdown est g√©n√©r√© et affich√©.
    - Cliquez sur **"Ouvrir la Toolbox"**.
    - Choisissez votre mode (`API` ou `Navigateur Int√©gr√©`).
    - Dans la Toolbox, cliquez sur **"Importer le contexte du projet"**.
    - Vous pouvez maintenant utiliser les prompts ou discuter avec l'IA √† propos de votre code.

## üíæ Gestion Avanc√©e des Conversations

La Toolbox va au-del√† d'un simple chat en proposant un syst√®me de sauvegarde complet, transformant chaque session en une "capsule temporelle" r√©utilisable.

### üéØ G√©n√©ration Automatique de Titres par IA
Lors de la sauvegarde d'une conversation, vous pouvez :
- **Saisir manuellement** un titre descriptif
- **Utiliser la baguette magique** ü™Ñ pour obtenir une suggestion de titre g√©n√©r√©e par l'IA qui analyse le contenu de votre conversation
- L'IA effectue une **analyse s√©mantique** en ignorant les blocs de code pour se concentrer sur le sujet principal de la discussion

### "Capsules Temporelles" de Conversation
Chaque sauvegarde n'enregistre pas seulement l'historique des messages, mais aussi **l'int√©gralit√© du contexte du projet** tel qu'il √©tait au moment de la conversation. Cela vous permet de reprendre une analyse ou un d√©veloppement exactement l√† o√π vous l'aviez laiss√©, m√™me si le code source a chang√© depuis.

### Syst√®me de Verrouillage Multi-Instance
Pour garantir l'int√©grit√© de vos donn√©es, un syst√®me de verrouillage intelligent est int√©gr√© :
-   **Verrouillage Automatique** : Lorsque vous chargez ou sauvegardez une conversation, elle est automatiquement "verrouill√©e" par votre session.
-   **Pr√©vention des Conflits** : Si vous ouvrez la m√™me conversation dans une autre fen√™tre, elle appara√Ætra comme verrouill√©e, vous emp√™chant de la modifier et d'√©craser accidentellement des donn√©es.
-   **Information Visuelle** : Des ic√¥nes claires indiquent le statut de chaque conversation (verrouill√©e par vous, par un autre, ou libre).
-   **Gestion des Verrous** : Vous pouvez lib√©rer manuellement vos verrous ou forcer la lib√©ration d'un verrou orphelin si une instance de l'application s'est mal ferm√©e.

### Fonctionnalit√©s de l'Interface
Depuis la barre lat√©rale de la Toolbox, vous pouvez :
-   **Sauvegarder** la conversation actuelle.
-   **Charger** une conversation existante pour restaurer l'historique et le contexte.
-   **Dupliquer** une conversation pour explorer une nouvelle piste d'analyse sans alt√©rer l'original.
-   **Renommer** vos conversations pour mieux les organiser.
-   **Supprimer** les sessions dont vous n'avez plus besoin.

## üèóÔ∏è Architecture Technique

Le projet adopte une architecture orient√©e services pour garantir la modularit√©, la testabilit√© et la clart√©. La classe `Api` dans `main_desktop.py` sert de **fa√ßade**, orchestrant les appels aux diff√©rents services backend.

- **`main_desktop.py` (API Fa√ßade)** : Point d'entr√©e de l'application de bureau. G√®re les fen√™tres (`pywebview`), expose les m√©thodes Python au JavaScript et orchestre les services.
- **`web_server.py` (Serveur Flask)** : Serveur local qui rend les templates HTML et fournit des endpoints API (principalement pour le mode web historique, mais utilis√© par la fen√™tre pywebview).
- **`services/` (Logique M√©tier)** :
    - `FileService` : G√®re le scan des syst√®mes de fichiers, l'application des r√®gles `.gitignore`, le filtrage des fichiers binaires et le **masquage des secrets**.
    - `ContextBuilderService` : Assemble le contexte final en Markdown, g√©n√®re l'arbre de fichiers et estime la taille en tokens.
    - `LlmApiService` : G√®re toute la communication avec les API LLM (OpenAI, Ollama), y compris la gestion du streaming et une strat√©gie de `retry` intelligente.
    - `GitService` : Ex√©cute les commandes Git, comme `git diff --staged`.
    - `ExportService` : G√®re l'export des conversations en Markdown, DOCX et PDF.
- **`pywebview_driver.py` (Pilote Personnalis√©)** : Un driver l√©ger imitant l'API de Selenium pour interagir par programmation avec le contenu de la fen√™tre de navigateur int√©gr√©e.
- **`tests/` (Suite de Tests)** : Le projet inclut des tests unitaires (`pytest`) pour chaque service ainsi que des tests d'int√©gration pour la fa√ßade `Api`, garantissant la robustesse de l'application.

## üîß Configuration Avanc√©e

Le fichier `config.ini` permet une personnalisation fine :

### Configuration de la G√©n√©ration de Titres par IA (`[TitleGeneratorLLM]`)

Configuration optionnelle pour personnaliser la g√©n√©ration automatique de titres. Si cette section est absente, le syst√®me utilise automatiquement la configuration de `[LLMServer]`.

```ini
[TitleGeneratorLLM]
# Activer/d√©sactiver la fonctionnalit√©
enabled = true

# Configuration sp√©cifique (optionnelle, utilise LLMServer si non d√©finie)
# url = YOUR_TITLE_LLM_API_URL_HERE
# apikey = YOUR_TITLE_LLM_API_KEY_HERE
# model = gpt-3.5-turbo  # Mod√®le plus l√©ger pour la g√©n√©ration de titres

# Prompt personnalis√© pour la g√©n√©ration
title_prompt = G√©n√®re un titre court et descriptif...

# Param√®tres de g√©n√©ration
max_title_length = 100
timeout_seconds = 15
# temperature = 0.5  # Plus d√©terministe pour les titres
```

### Exclusion de Fichiers (`[FileExclusion]`)

Excluez des fichiers ou des motifs de la s√©lection.

```ini
[FileExclusion]
# Fichiers sp√©cifiques √† exclure, s√©par√©s par des virgules
file_blacklist = .DS_Store, Thumbs.db, yarn.lock
# Motifs √† exclure (supporte * et ?)
pattern_blacklist = *.min.js, *-lock.json, *.pyc
```

### D√©tection Binaire (`[BinaryDetection]`)

Affinez la d√©tection des fichiers binaires.

```ini
[BinaryDetection]
# Extensions imm√©diatement rejet√©es
extension_blacklist = .png, .jpg, .exe, .dll, .so, .pdf, .zip, .woff
# Extensions imm√©diatement accept√©es sans analyse de contenu
extension_whitelist = .py, .js, .html, .css, .json, .md, .txt, .sh
```

## üÜï Nouvelles Fonctionnalit√©s (D√©cembre 2024)

### ‚ö° Gestion Intelligente des Erreurs R√©seau

**Syst√®me de Retry avec Circuit Breaker** :
- **Retry automatique** : Jusqu'√† 6 tentatives avec backoff exponentiel (1s ‚Üí 30s)
- **Circuit breaker** : D√©sactive temporairement les endpoints d√©faillants apr√®s 3 √©checs
- **Failover intelligent** : Bascule automatiquement entre les mod√®les LLM configur√©s
- **Notifications visuelles** : Affichage en temps r√©el des tentatives et erreurs
- **R√©cup√©ration automatique** : R√©activation progressive des endpoints apr√®s 2 minutes

Cette fonctionnalit√© est particuli√®rement utile pour g√©rer :
- Les erreurs 504 Gateway Timeout
- Les surcharges serveur
- Les probl√®mes de connectivit√© r√©seau
- Les limites de quota atteintes

### üåê Support Proxy d'Entreprise

**Configuration proxy pour acc√©der aux LLM externes** :

```ini
[LLM:OpenAI-External]
url = https://api.openai.com/v1
apikey = YOUR_API_KEY
model = gpt-4
enabled = true
# Configuration proxy pour sortir sur Internet
proxy_http = http://proxy.entreprise.com:8080
proxy_https = http://proxy.entreprise.com:8080
proxy_no_proxy = localhost,127.0.0.1,.entreprise.local
```

**Caract√©ristiques** :
- Support proxy HTTP/HTTPS avec authentification
- Configuration ind√©pendante par mod√®le LLM
- Exclusions via `proxy_no_proxy`
- Compatible avec SummarizerLLM et TitleGeneratorLLM
- Script de diagnostic inclus : `python test_proxy_llm.py`

**Cas d'usage** : Permet de basculer vers des services externes (OpenAI, Anthropic) lorsque les quotas internes sont atteints.

### üìä Monitoring et Sant√© des Endpoints

- **Tableau de bord de sant√©** : Visualisation du statut de chaque endpoint LLM
- **M√©triques** : Taux de succ√®s, nombre de requ√™tes, derni√®re erreur
- **API de statut** : R√©cup√©ration programmatique de l'√©tat des services
- **R√©initialisation manuelle** : Possibilit√© de r√©activer un endpoint

### üõ†Ô∏è Outils de D√©veloppement

**Scripts de test inclus** :
- `test_retry_manager.py` : Test du syst√®me de retry et failover
- `test_proxy_llm.py` : V√©rification de la configuration proxy
- `test_llm_error_display.py` : Test des notifications d'erreur
- `test_error_notification.html` : Interface de test des notifications visuelles

**Documentation technique** :
- `docs/retry-failover-strategy.md` : Guide complet du syst√®me de retry
- `docs/proxy-configuration.md` : Configuration d√©taill√©e du proxy avec exemples

## üîß D√©pannage

### Erreurs fr√©quentes

**Erreur 504 Gateway Timeout** :
- Le syst√®me de retry prend automatiquement le relais
- V√©rifiez que plusieurs mod√®les LLM sont configur√©s pour le failover
- Augmentez `timeout_seconds` si n√©cessaire

**Erreur de connexion via proxy** :
- V√©rifiez la configuration proxy dans `config.ini`
- Ex√©cutez `python test_proxy_llm.py` pour diagnostiquer
- Assurez-vous que le domaine n'est pas bloqu√© par la politique d'entreprise

**Limite de tokens atteinte** :
- Basculez vers un autre mod√®le LLM via le s√©lecteur de l'interface
- Configurez un mod√®le externe avec proxy si n√©cessaire

## ü§ù Contribution

Les contributions sont les bienvenues ! Si vous souhaitez am√©liorer l'application, n'h√©sitez pas √† forker le d√©p√¥t et √† soumettre une Pull Request.

## üìÑ Licence

Ce projet est sous licence MIT. Voir le fichier `LICENSE` pour plus de d√©tails.
