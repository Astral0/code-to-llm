# üîç LLM Context Builder

A tool for preparing code repositories for LLM analysis and modification. Create concise, properly formatted context from your projects that can be easily shared with LLMs.

> üõ°Ô∏è **Security Feature**: Automatically detects and masks sensitive information like API keys, credentials, and tokens to prevent them from being shared with LLMs.

## üöÄ Features

- **Smart Repository Scanning**: Recursively scans directories, automatically respects .gitignore rules
- **Proper Formatting**: Creates neatly formatted code blocks with language detection
- **Directory Tree Visualization**: Generates a visual tree structure of your project
- **Web Interface**: Simple browser UI for selecting files and generating context
- **CLI Support**: Command-line interface for automation and scripts
- **Sensitive Data Protection**: Detects and masks API keys, passwords, tokens and other credentials
- **Token Estimation**: Provides approximate token count for LLM context windows

### Web Interface Features

![image](https://github.com/user-attachments/assets/e2816992-9967-403a-b8d6-3df66e618a0e)

- **File Selection**: Interactively select files and folders from your chosen directory.
- **.gitignore Integration**: Automatically respects `.gitignore` rules to exclude irrelevant files.
- **Custom Instructions**: Add specific instructions to be appended to the end of the generated context, guiding the LLM on subsequent tasks.
  - Includes buttons for quickly inserting predefined, configurable instruction templates.
  - Smart insertion logic: appends to existing custom text, or replaces the last predefined instruction if one was just inserted.
- **Context Regeneration**: Easily regenerate the context. If files have been modified, you'll be guided to re-select the directory to ensure all changes are captured, while your previous file selection is preserved.
- **Secret Masking Toggle**: Enable or disable sensitive data masking directly from the UI.
- **Copy to Clipboard**: Quickly copy the generated Markdown context.
- **LLM Integration (Configurable)**:
  - Send the generated context directly to a configured LLM API (OpenAI compatible or Ollama).
  - View the LLM's response directly in the interface.
  - Chat with the LLM with conversation history.
  - LLM responses are rendered as Markdown in the chat.
  - Quickly append predefined instructions (e.g., for generating patches) to your chat messages.
  - Copy the LLM's response to the clipboard.
  - Configuration via `config.ini` for API endpoint, key, model, API type, and streaming.

## ‚öôÔ∏è Installation

### Requirements

- Python 3.8+
- Flask
- pathspec
- detect-secrets

### Setup

```bash
git clone https://github.com/Astral0/code-to-llm.git
cd code-to-llm
pip install -r requirements.txt
```

The web interface allows for configuration of predefined instruction templates and LLM server settings via a `config.ini` file at the root of the project. 
Create `config.ini` from `config.ini.template` and customize it:

```ini
[Instructions]
instruction1_text = Your first predefined instruction.
instruction2_text = Your second predefined instruction.

[LLMServer]
url = YOUR_LLM_API_URL_HERE # e.g., https://api.openai.com/v1/chat/completions or http://localhost:11434
                             # For OpenAI, can be base 'https://api.openai.com' or full '.../v1/chat/completions'.
                             # For Ollama, can be base 'http://host:port' or include '/api/chat' or '/v1/chat/completions'.
apikey = YOUR_LLM_API_KEY_HERE # Optional for local Ollama, required for OpenAI
model = YOUR_LLM_MODEL_HERE   # e.g., gpt-3.5-turbo or llama3
api_type = openai             # 'openai' or 'ollama'
enabled = false               # Set to true to enable LLM interaction features
stream_response = false       # Set to true to enable streaming responses from the LLM

[Tokens]
```

### Web Interface

```bash
python web_server.py # Default port 5000
# or specify a custom port, e.g., 8080
python web_server.py --port 8080
```

Then open http://127.0.0.1:5000 (or your custom port) in your browser.

### Command Line

```bash
python llm_context_builder.py cli path/to/your/project -o output.md
```

### CLI Options

```bash
# General
python llm_context_builder.py cli <paths_to_scan...>

# Output Configuration
  -o, --output-file <file>    Specify the output file for the context. (Default: stdout)
  --no-tree                   Disable the generation of the directory tree visualization.
  --no-header                 Disable the standard context header.
  --no-footer                 Disable the standard file footers (--- END FILE ---).

# Content Filtering & Handling
  --repo-root <path>          Specify the project root if different from the scanned path.
                              Used for more accurate relative paths in the context.
  --exclude <pattern>         Specify additional glob patterns to exclude files/directories.
                              Can be used multiple times. (e.g., --exclude "*.log" --exclude "temp/")
  --include-binary            Include a short header of binary files instead of skipping them.
  --max-size <size>           Maximum size for individual files (e.g., 1M, 500K). Files exceeding this
                              will be truncated or skipped (behavior may vary).
  --encoding <enc>            Specify encoding for reading files (e.g., utf-8, latin-1).
                              (Default: utf-8 with error handling)

# Security & Masking
  --no-masking                Disable all sensitive data masking.
  --masking-mode <mode>       Set masking mode: 'mask' (replace with placeholder, default) 
                              or 'remove' (remove the line containing the secret).

# Instructions
  --instructions "<text>"   Add custom instructions to the end of the generated context.

# Other
  --debug                     Enable debug logging for verbose output.
  --no-progress               Disable the progress bar during file processing.
```

### Advanced Options

```bash
# Specify a custom repository root
python llm_context_builder.py cli path/to/your/project -o output.md --repo-root /custom/root --debug

# Start web server on a specific port
python llm_context_builder.py serve --port 8080 --host 0.0.0.0
```

## üõ°Ô∏è Security Features

### Sensitive Data Masking

The tool automatically detects and masks sensitive information in your code, including:

- API keys and tokens
- Passwords and credentials
- Private keys and certificates
- Connection strings with embedded credentials
- AWS keys and other cloud provider credentials

When sensitive data is detected, it will be replaced with a masked indicator such as:

```
[LINE CONTAINING SENSITIVE DATA: ArtifactoryDetector]
```

Or, depending on the detection method and file type:
```
[LINE REMOVED DUE TO DETECTED SECRET]
```
Or sometimes simply, for very sensitive lines:
```
[LINE CONTAINING SENSITIVE DATA: some_pattern_name]
```

> **Note**: This masking helps prevent accidental leakage of sensitive information when sharing code context with LLMs. You can enable or disable this feature in the web interface.

### Detection Methods

The security scanning uses two complementary approaches:

1. **detect-secrets library**: Uses a variety of detectors for different types of secrets
2. **Custom regex patterns**: Additional patterns for common credential formats

## üîÑ Output Format

The generated context includes:

1. A header with project information
2. A directory tree visualization
3. The content of each file with language-specific formatting
4. Custom instructions, if provided by the user

```
--- START CONTEXT ---
Objective: Provide the complete context of a project to enable an LLM to understand the code and apply user-requested modifications.
Structure: First, the project's directory tree is presented, followed by the full content of each relevant file.
Security Note: Sensitive information such as API keys, tokens, passwords, and credentials have been automatically masked in this context.
...

--- START DIRECTORY TREE ---
Project_Root/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ main.py
‚îÇ   ‚îî‚îÄ‚îÄ utils.py
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îî‚îÄ‚îÄ test_main.py
‚îî‚îÄ‚îÄ README.md
--- END DIRECTORY TREE ---

--- START FILE: src/main.py ---
```python
# Main application code
...
```
--- END FILE: src/main.py ---

...
--- INSTRUCTIONS ---
Refactor the main_function to improve readability.
--- END INSTRUCTIONS ---

## üí° Tips for Using with LLMs

- **Prompt Structure**: Provide the context followed by clear instructions about what modifications you need
- **File References**: Refer to files using their exact paths as shown in the context
- **Token Management**: For large projects, select only relevant files to stay within LLM context limits
- **Security Check**: Always verify that sensitive data has been properly masked before sharing

## ü§ù Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## üìÑ License

This project is licensed under the MIT License - see the LICENSE file for details.

## Mode d'Emploi de l'Application

### Mode Web (pour un environnement standard)

Pour lancer l'application en mode web, suivez ces instructions :

1.  Lancez le serveur Flask en ex√©cutant la commande `python web_server.py`.
2.  Ouvrez un navigateur web √† l'adresse `http://127.0.0.1:5000`.
3.  Le pilotage du navigateur externe se fait via des scripts Selenium s√©par√©s, comme dans le fonctionnement original.

### Mode Bureau (pour un environnement restreint)

Pour lancer l'application en mode bureau, suivez ces instructions :

1.  Lancez l'application de bureau en ex√©cutant le script `run_desktop.bat`.
2.  Cela ouvrira une fen√™tre d'application native affichant l'interface.
3.  Le bouton "D√©marrer la discussion avec le LLM" ouvrira une seconde fen√™tre de navigateur pour l'automatisation.

### Utilisation des scripts
Pour faciliter l'ex√©cution des commandes, des scripts `.bat` sont fournis :
- `run_serve.bat` : Lance le serveur web (en fran√ßais).
- `run_serve_en.bat` : Lance le serveur web (en anglais).
- `run_desktop.bat` : Lance l'application de bureau.
